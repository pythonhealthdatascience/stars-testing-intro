---
title: "System tests"
---

```{r}
#| include: false
library(reticulate)
use_condaenv("stars-testing-intro", required = TRUE)
```

{{< include /assets/language-selector.html >}}

<br>

## What is a system test?

**System** tests are broader than unit tests. They focus on a whole feature or workflow, often involving several functions or classes working together. They check whether this end-to-end behaviour gives the correct outputs for given inputs, matching our requirements or expected results.

::: {.callout-note title="Integration tests"}

Another useful type of test that sits between unit and system tests is the **integration test**.

Integration tests focus on how two or more components work together (e.g., how a data import function hands data to a processing function), without necessarily running the whole workflow end-to-end.

In this small case study, adding separate integration tests would not add much beyond our unit and system tests, but in larger projects they are very helpful for checking the interactions between parts of your code.

:::

## Example: waiting times case study

We will return to our [waiting times case study](case_study.qmd) which involved three functions:

* `import_patient_data()` - imports raw patient data and checks that the required columns are present.
* `calculate_wait_times()` - adds arrival and service datetimes, and waiting time in minutes.
* `summary_stats()` - calculates mean, standard deviation and 95% confidence interval.

Unlike unit tests, which check each function in isolation, **system tests** run all three steps together and verify the end‑to‑end workflow produces correct results.

::: {.python-content}

We will need the follow imports in our test script:

```{python}
#| eval: false
#| file: code/test_system__imports.py
```

:::

## How to write system tests

<div class="h3-tight"></div>

### 1. Identify the feature or workflow to test

Start by choosing the complete feature or workflow you want to validate.

In our case study, we only have a simple three-step pipeline - but more complex projects may have multiple intersecting workflows you want to focus on.

### 2. Define inputs and expected outputs

Think about realistic scenarios that cover:

* **Clean/positive/success cases:** standard inputs where everything should work correctly.
* **Variations:** realistic variations in the inputs (e.g., different sample sizes, different distributions in the data input).
* **Edge/extreme cases:** unusual but plausible inputs (e.g., unusual sample sizes, boundary values).
* **Error/negative/dirty cases:** invalid inputs that should trigger errors.

### 3. Write tests for these scenarios

#### Clean case: typical data

This test confirms the workflow succeeds with standard inputs and produces correct summary statistics.

::: {.python-content}

```{python}
#| file: code/test_system__test_workflow_success.py
#| eval: false
```

:::

::: {.r-content}

```{r}
#| file: code/test_system__complete_workflow_should.R
#| eval: false
```

:::

#### Variation: data with different distributions

This test confirms the workflow handles realistic variation in wait times.

::: {.python-content}

```{python}
#| file: code/test_system__test_workflow_with_variation.py
#| eval: false
```

:::

::: {.r-content}

```{r}
#| file: code/test_system__workflow_should_give.R
#| eval: false
```

:::

#### Error case: invalid input data

This test confirms the workflow fails appropriately when given invalid data.

::: {.python-content}

```{python}
#| file: code/test_system__test_missing_date_error.py
#| eval: false
```

:::

::: {.r-content}

```{r}
#| file: code/test_system__workflow_should_raise.R
#| eval: false
```

:::

## Running our example tests

:::: {.callout-note title="Test output"}

::: {.python-content}

```{python}
#| echo: false
import pytest
pytest.main(["../examples/python_package/tests/test_system.py"])
```

:::

::: {.r-content}

```{r}
#| echo: false
#| output: false
devtools::load_all("../examples/r_package")
```

```{r}
#| echo: false
testthat::test_file(
  "../examples/r_package/tests/testthat/test_system.R"
)
```

:::

::::

## When to stop writing tests

You cannot test everything. You've written enough tests when:

* Critical workflows are covered with at least one success case.
* Import variations and edge cases are tested.
* Key error conditions are verified.

Focus your testing effort on workflows that matter to your research and scenarios you're likely to encounter in practice. You're building confidence in your code, not trying to test every theoretical possibility.
