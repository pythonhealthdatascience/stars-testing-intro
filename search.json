[
  {
    "objectID": "pages/parametrising_tests.html",
    "href": "pages/parametrising_tests.html",
    "title": "Parameterising tests",
    "section": "",
    "text": "Choose your language:¬†¬†\n    Python\n    R\nThere are many tools you can make use of when testing - one example is parametrising tests.\nWhen you need to test the same logic with different inputs and expected outputs, you can parameterise your tests instead of writing repetitive test functions. This minimises code duplication and makes it easy to add new test cases.",
    "crumbs": [
      "Introduction to writing and running tests",
      "Parameterising tests"
    ]
  },
  {
    "objectID": "pages/parametrising_tests.html#example-testing-summary_stats",
    "href": "pages/parametrising_tests.html#example-testing-summary_stats",
    "title": "Parameterising tests",
    "section": "Example: Testing summary_stats()",
    "text": "Example: Testing summary_stats()\nLet‚Äôs say we want to verify that our summary_stats() function works correctly for different datasets.\n\n\n\n\n\n\nNoteView summary_stats()\n\n\n\n\n\n\n\ndef summary_stats(data):\n    \"\"\"\n    Calculate mean, standard deviation and 95% confidence interval (CI).\n\n    CI is calculated using the t-distribution, which is appropriate for\n    small samples and converges to the normal distribution as the sample\n    size increases.\n\n    Parameters\n    ----------\n    data : pandas.Series\n        Data to use in the calculation.\n\n    Returns\n    -------\n    dict[str, float]\n        A dictionary with keys `mean`, `std_dev`, `ci_lower` and `ci_upper`.\n        Each value is a float, or `numpy.nan` if it can't be computed.\n    \"\"\"\n    # Drop missing values\n    data = data.dropna()\n\n    # Find number of observations\n    count = len(data)\n\n    # If there are no observations, then set all to NaN\n    if count == 0:\n        mean, std_dev, ci_lower, ci_upper = np.nan, np.nan, np.nan, np.nan\n\n    # If there are 1 or 2 observations, can do mean but not other statistics\n    elif count &lt; 3:\n        mean = data.mean()\n        std_dev, ci_lower, ci_upper = np.nan, np.nan, np.nan\n\n    # With more than two observations, can calculate all...\n    else:\n        mean = data.mean()\n        std_dev = data.std()\n\n        # If there is no variation, then CI is equal to the mean\n        if np.var(data) == 0:\n            ci_lower, ci_upper = mean, mean\n        else:\n            # 95% CI based on the t-distribution\n            ci_lower, ci_upper = st.t.interval(\n                confidence=0.95,\n                df=count-1,\n                loc=mean,\n                scale=st.sem(data)\n            )\n\n    return {\n        \"mean\": mean,\n        \"std_dev\": std_dev,\n        \"ci_lower\": ci_lower,\n        \"ci_upper\": ci_upper\n    }\n\n\n\n\n#' Calculate mean, standard deviation and 95% confidence interval (CI).\n#'\n#' CI is calculated using the t-distribution, which is appropriate for\n#' small samples and converges to the normal distribution as the sample\n#' size increases.\n#'\n#' @param data Numeric vector of data to use in the calculation.\n#'\n#' @return A named list with elements `mean`, `std_dev`, `ci_lower` and \n#'   `ci_upper`. Each value is a numeric, or `NA` if it can't be computed.\n#'\n#' @export\nsummary_stats &lt;- function(data) {\n  tibble::tibble(value = data) |&gt;\n    dplyr::reframe(\n      n_complete = sum(!is.na(value)),\n      mean = mean(value, na.rm = TRUE),\n      std_dev = stats::sd(value, na.rm = TRUE),\n      ci_lower   = {\n        if (n_complete &lt; 2L) {\n          NA_real_\n        } else if (std_dev == 0 || is.na(std_dev)) {\n          mean       # CI collapses to mean when no variation\n        } else {\n          stats::t.test(value)$conf.int[1L]\n        }\n      },\n      ci_upper   = {\n        if (n_complete &lt; 2L) {\n          NA_real_\n        } else if (std_dev == 0 || is.na(std_dev)) {\n          mean       # CI collapses to mean when no variation\n        } else {\n          stats::t.test(value)$conf.int[2L]\n        }\n      }\n    ) |&gt;\n    dplyr::select(-n_complete) |&gt;\n    as.list()\n}\n\n\n\n\n\n\nWe will need the following imports in our test script:\n\nimport pandas as pd\nimport pytest\nfrom waitingtimes.patient_analysis import summary_stats\n\nInstead of writing separate test functions for each case, we can use pytest‚Äôs @pytest.mark.parametrize decorator:\n\ndef test_summary_stats(\n    data, expected_mean, expected_std, expected_ci_lower, expected_ci_upper\n):\n    \"\"\"Running summary_stats returns expected values.\"\"\"\n    res = summary_stats(pd.Series(data))\n    assert res[\"mean\"] == pytest.approx(expected_mean, rel=5e-3)\n    assert res[\"std_dev\"] == pytest.approx(expected_std, rel=5e-3)\n    assert res[\"ci_lower\"] == pytest.approx(expected_ci_lower, rel=5e-3)\n    assert res[\"ci_upper\"] == pytest.approx(expected_ci_upper, rel=5e-3)\n\n\nHow it works\nThe @pytest.mark.parametrize decorator takes two arguments:\n\nParameter names (as a string). These variable names will be passed to your test function. For example:\n\n\"data, expected_mean, expected_std, expected_ci_lower, expected_ci_upper\"\n\nTest cases (as a list of tuples). Each tuple contains values for one test case. For example:\n\n[\n    # Five value sample with known summary statistics\n    ([1.0, 2.0, 3.0, 4.0, 5.0], 3.0, 1.58, 1.04, 4.96),\n    # No variation: CI collapse to mean\n    ([5, 5, 5], 5, 0, 5, 5),\n]\nIf any test case fails, pytest will clearly indicate which parameters were used, making debugging straightforward.\n\n\n\n\nInstead of test_that(), we will use the function with_parameters_test_that() from Google‚Äôs patrick package. This lets us write our test code once, then provide a list of input/output combinations (the ‚Äúcases‚Äù) that are run through the same test code.\nThe general pattern is:\npatrick::with_parameters_test_that(\n  \"Description of test\",\n  {\n    # Test code using the parameters e.g., expect_...\n  },\n  patrick::cases(\n    list(input1 = 5L, input2 = 10L, output = 500L),\n    list(input1 = 6L, input2 = 11L, output = 600L)\n  )\n)\nEach list() inside cases() defines one test case, with named elements matching the arguments used inside the code block.\n\nFor summary_stats, we can write our test as:\n\npatrick::with_parameters_test_that(\n  \"summary_stats returns expected values\",\n  {\n    res &lt;- summary_stats(data)\n\n    expect_equal(res$mean, expected_mean, tolerance = 5e-3)\n    expect_equal(res$std_dev, expected_std, tolerance = 5e-3)\n    expect_equal(res$ci_lower, expected_ci_lower, tolerance = 5e-3)\n    expect_equal(res$ci_upper, expected_ci_upper, tolerance = 5e-3)\n  },\n  patrick::cases(\n    # Five value sample with known summary statistics\n    list(\n      data = c(1.0, 2.0, 3.0, 4.0, 5.0),\n      expected_mean = 3.0,\n      expected_std = 1.58,\n      expected_ci_lower = 1.04,\n      expected_ci_upper = 4.96\n    ),\n    # No variation: CI collapse to mean\n    list(\n      data = c(5, 5, 5),\n      expected_mean = 5,\n      expected_std = 0,\n      expected_ci_lower = 5,\n      expected_ci_upper = 5\n    )\n  )\n)\n\nWhen this test runs, with_parameters_test_that() executes the code block once for each case, substituting in the corresponding data and expected values.",
    "crumbs": [
      "Introduction to writing and running tests",
      "Parameterising tests"
    ]
  },
  {
    "objectID": "pages/parametrising_tests.html#how-it-works",
    "href": "pages/parametrising_tests.html#how-it-works",
    "title": "Parameterising tests",
    "section": "How it works",
    "text": "How it works\nThe @pytest.mark.parametrize decorator takes two arguments:\n\nParameter names (as a string). These variable names will be passed to your test function. For example:\n\n\"data, expected_mean, expected_std, expected_ci_lower, expected_ci_upper\"\n\nTest cases (as a list of tuples). Each tuple contains values for one test case. For example:\n\n[\n    # Five value sample with known summary statistics\n    ([1.0, 2.0, 3.0, 4.0, 5.0], 3.0, 1.58, 1.04, 4.96),\n    # No variation: CI collapse to mean\n    ([5, 5, 5], 5, 0, 5, 5),\n]\nIf any test case fails, pytest will clearly indicate which parameters were used, making debugging straightforward.",
    "crumbs": [
      "Introduction to writing and running tests",
      "Parameterising tests"
    ]
  },
  {
    "objectID": "pages/parametrising_tests.html#running-our-test",
    "href": "pages/parametrising_tests.html#running-our-test",
    "title": "Parameterising tests",
    "section": "Running our test",
    "text": "Running our test\n\n\n\n\n\n\nNoteTest output\n\n\n\n\n\n\n============================= test session starts ==============================\nplatform linux -- Python 3.12.12, pytest-9.0.2, pluggy-1.6.0\nrootdir: /__w/hdruk_tests/hdruk_tests/examples/python_package\nconfigfile: pyproject.toml\nplugins: cov-7.0.0\ncollected 2 items\n\n../examples/python_package/tests/test_intro_parametrised.py ..           [100%]\n\n============================== 2 passed in 0.91s ===============================\n&lt;ExitCode.OK: 0&gt;\n\n\n\n\n\n\n\n‚ïê‚ïê Testing test_intro_parametrised.R ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 0 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 3 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 4 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 5 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 6 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 7 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 8 ] Done!",
    "crumbs": [
      "Introduction to writing and running tests",
      "Parameterising tests"
    ]
  },
  {
    "objectID": "pages/unit_tests.html",
    "href": "pages/unit_tests.html",
    "title": "Unit tests",
    "section": "",
    "text": "Choose your language:¬†¬†\n    Python\n    R",
    "crumbs": [
      "Types of test",
      "Unit tests"
    ]
  },
  {
    "objectID": "pages/unit_tests.html#what-is-a-unit-test",
    "href": "pages/unit_tests.html#what-is-a-unit-test",
    "title": "Unit tests",
    "section": "What is a unit test?",
    "text": "What is a unit test?\nA unit test checks one small, isolated unit of code - usually a single function or method. Your aim is to check that, for specific inputs, the function behaves exactly as promised.",
    "crumbs": [
      "Types of test",
      "Unit tests"
    ]
  },
  {
    "objectID": "pages/unit_tests.html#example-import_patient_data",
    "href": "pages/unit_tests.html#example-import_patient_data",
    "title": "Unit tests",
    "section": "Example: import_patient_data()",
    "text": "Example: import_patient_data()\n\nLet‚Äôs use the import_patient_data() function from our case study. We will import it to our test script, alongside other required packages.\n\nimport pandas as pd\nimport pytest\nfrom waitingtimes.patient_analysis import import_patient_data\n\nIts main behaviours are that it:\n\nReads a CSV into a pandas DataFrame.\nRequires the columns to match a specific list exactly (names and order).\nRaises a ValueError if columns are incorrect.\nReturns a DataFrame with raw patient-level data.\n\n\n\nLet‚Äôs use the import_patient_data() function from our case study.\nIts main behaviours are that it:\n\nReads a CSV into a dataframe.\nRequires the columns to match a specific list exactly (names and order).\nStops if columns are incorrect.\nReturns a dataframe with raw patient-level data.\n\n\n\n\n\n\n\n\nNoteView import_patient_data()\n\n\n\n\n\n\n\ndef import_patient_data(path):\n    \"\"\"\n    Import raw patient data and check that required columns are present.\n\n    Parameters\n    ----------\n    path : str or pathlib.Path\n        Path to the CSV file containing the patient data.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Dataframe containing the raw patient-level data.\n\n    Raises\n    ------\n    ValueError\n        If the CSV file does not contain exactly the expected columns\n        in the expected order.\n    \"\"\"\n    df = pd.read_csv(Path(path))\n\n    # Expected columns in the raw data (names and order must match)\n    expected = [\n        \"PATIENT_ID\",\n        \"ARRIVAL_DATE\", \"ARRIVAL_TIME\",\n        \"SERVICE_DATE\", \"SERVICE_TIME\"\n    ]\n    if list(df.columns) != expected:\n        raise ValueError(\n            f\"Unexpected columns: {list(df.columns)} (expected {expected})\"\n        )\n\n    return df\n\n\n\n\n#' Import raw patient data and check that required columns are present.\n#'\n#' Raises an error if the CSV file does not contain exactly the expected \n#' columns in the expected order.\n#'\n#' @param path Character string giving path to the CSV file containing the \n#'   patient data.\n#'\n#' @return A data frame containing the raw patient-level data.\n#'\n#' @export\nimport_patient_data &lt;- function(path) {\n  df &lt;- readr::read_csv(path, show_col_types = FALSE)\n\n  # Expected columns in the raw data (names and order must match)\n  expected &lt;- c(\n    \"PATIENT_ID\",\n    \"ARRIVAL_DATE\", \"ARRIVAL_TIME\",\n    \"SERVICE_DATE\", \"SERVICE_TIME\"\n  )\n  if (!identical(colnames(df), expected)) {\n    stop(\n      sprintf(\n        \"Unexpected columns: %s (expected %s)\",\n        paste(colnames(df), collapse = \", \"),\n        paste(expected, collapse = \", \")\n      )\n    )\n  }\n\n  return(df)\n}",
    "crumbs": [
      "Types of test",
      "Unit tests"
    ]
  },
  {
    "objectID": "pages/unit_tests.html#how-to-write-unit-tests",
    "href": "pages/unit_tests.html#how-to-write-unit-tests",
    "title": "Unit tests",
    "section": "How to write unit tests",
    "text": "How to write unit tests\n\n\n\n\n1. Start from the docstring\nAlways write docstrings for your code (see our docstring tutorial if you need guidance).\nYour docstring makes promises about how the function should behave. For import_patient_data(), the docstring promises it will:\n\n\nAccept str or Path as the path parameter.\nReturn a pandas DataFrame.\nRaise ValueError if columns are incorrect.\n\n\n\n\nReturn a dataframe.\nStop if columns are incorrect.\n\n\nEach of these becomes something you can check with a test.\n\n\n2. Define what ‚Äúsuccess‚Äù looks like\nPick the simplest input that should work.\nIn our case, we can create a small CSV with correct columns in the right order and one or two data rows. We can then write tests that confirm:\n\n\nThe result is a pandas DataFrame.\nThe columns match exactly: list(df.columns) == expected_list\n\n\n\n\nThe result is a dataframe.\nThe columns match exactly.\n\n\n\n\n\n\n\n\nNoteWhy test columns if the function already checks them?\n\n\n\nBecause we are testing the promised behaviour of the function, not just trusting its current implementation.\nIf someone edits the code later and accidentally removes that validation, your test will catch it!\n\n\n\n\ndef test_import_success(tmp_path):\n    \"\"\"Small CSV with correct columns should work.\"\"\"\n\n    expected_cols = [\n        \"PATIENT_ID\", \"ARRIVAL_DATE\", \"ARRIVAL_TIME\",\n        \"SERVICE_DATE\", \"SERVICE_TIME\",\n    ]\n\n    # Create temporary CSV file\n    df_in = pd.DataFrame(\n        [[\"p1\", \"2024-01-01\", \"08:00\", \"2024-01-01\", \"09:00\"]],\n        columns=expected_cols,\n    )\n    csv_path = tmp_path / \"patients.csv\"\n    df_in.to_csv(csv_path, index=False)\n\n    # Run function and check it looks correct\n    result = import_patient_data(csv_path)\n    assert isinstance(result, pd.DataFrame)\n    assert list(result.columns) == expected_cols\n    pd.testing.assert_frame_equal(result, df_in)\n\n\n\n\ntest_that(\"small CSV with correct columns imports successfully\", {\n  expected_cols &lt;- c(\n    \"PATIENT_ID\", \"ARRIVAL_DATE\", \"ARRIVAL_TIME\",\n    \"SERVICE_DATE\", \"SERVICE_TIME\"\n  )\n\n  # Create temporary CSV file\n  df_in &lt;- tibble::tibble(\n    PATIENT_ID   = \"p1\",\n    ARRIVAL_DATE = lubridate::ymd(\"2024-01-01\"),\n    ARRIVAL_TIME = hms::as_hms(\"08:00:00\"),\n    SERVICE_DATE = lubridate::ymd(\"2024-01-01\"),\n    SERVICE_TIME = hms::as_hms(\"09:00:00\")\n  )\n  csv_path &lt;- tempfile(fileext = \".csv\")\n  readr::write_csv(df_in, csv_path)\n\n  # Run function and check it looks correct\n  result &lt;- import_patient_data(csv_path)\n  expect_s3_class(result, \"data.frame\")\n  expect_identical(names(result), expected_cols)\n  expect_equal(as.data.frame(result), as.data.frame(df_in))\n})\n\n\n\n\n3. List ways things can go wrong\nNow think: how can inputs break the promises?\n\nFor import_patient_data(), a ValueError should be raised when we have:\n\nMissing columns\nExtra columns\nCorrect columns but wrong order\n\nFor each case, we can create a small DataFrame with the problem and assert that a ValueError is raised.\n\ndef test_import_errors(tmp_path, columns):\n    \"\"\"Incorrect columns should trigger ValueError.\"\"\"\n\n    # Create temporary CSV file\n    df_in = pd.DataFrame([range(len(columns))], columns=columns)\n    csv_path = tmp_path / \"patients.csv\"\n    df_in.to_csv(csv_path, index=False)\n\n    # Check it raises ValueError\n    with pytest.raises(ValueError):\n        import_patient_data(csv_path)\n\n\n\nFor import_patient_data(), the function should stop with an error if we have:\n\nMissing columns\nExtra columns\nCorrect columns but wrong order\n\nFor each case, we can create a small dataframe with the problem and check that the function fails.\n\npatrick::with_parameters_test_that(\n  \"incorrect columns cause import_patient_data() to fail\",\n  {\n    # Create dataframe with incorrect columns\n    df &lt;- as.data.frame(as.list(seq_along(cols)))\n    names(df) &lt;- cols\n    # Save as temporary CSV and run function, expecting an error\n    csv_path &lt;- tempfile(fileext = \".csv\")\n    readr::write_csv(df, csv_path)\n    expect_error(import_patient_data(csv_path))\n  },\n  patrick::cases(\n    # Example 1: Missing columns\n    list(\n      cols = c(\"PATIENT_ID\", \"ARRIVAL_DATE\", \"ARRIVAL_TIME\", \"SERVICE_DATE\")\n    ),\n    # Example 2: Extra columns\n    list(\n      cols = c(\n        \"PATIENT_ID\", \"ARRIVAL_DATE\", \"ARRIVAL_TIME\", \"SERVICE_DATE\",\n        \"SERVICE_TIME\", \"EXTRA\"\n      )\n    ),\n    # Example 3: Right columns, wrong order\n    list(\n      cols = c(\n        \"ARRIVAL_DATE\", \"PATIENT_ID\", \"ARRIVAL_TIME\",\n        \"SERVICE_DATE\", \"SERVICE_TIME\"\n      )\n    )\n  )\n)\n\n\n\n\n4. Consider edge cases\nEdge cases are inputs that are unusual but still realistic.\nFor example, what if the CSV has the correct headers but no data? Should that succeed and return an empty DataFrame, or should it fail?\nIn this case, you might decide that an empty CSV with correct headers is fine and does not raise an error. You may still choose to write a test though, as that makes this decision explicit so other coders know what ‚Äúcorrect‚Äù means at the edges.\n\n\ndef test_import_empty_csv(tmp_path):\n    \"\"\"Empty CSV with correct columns should succeed.\"\"\"\n\n    expected_cols = [\n        \"PATIENT_ID\", \"ARRIVAL_DATE\", \"ARRIVAL_TIME\",\n        \"SERVICE_DATE\", \"SERVICE_TIME\",\n    ]\n\n    # Create empty CSV with correct header\n    df_in = pd.DataFrame(columns=expected_cols)\n    csv_path = tmp_path / \"patients.csv\"\n    df_in.to_csv(csv_path, index=False)\n\n    # Should succeed and return empty DataFrame\n    result = import_patient_data(csv_path)\n    assert len(result) == 0\n    assert list(result.columns) == expected_cols\n\n\n\n\ntest_that(\"empty CSV with correct columns should succeed\", {\n  # Empty CSV with correct columns should succeed.\n\n  expected_cols &lt;- c(\n    \"PATIENT_ID\", \"ARRIVAL_DATE\", \"ARRIVAL_TIME\",\n    \"SERVICE_DATE\", \"SERVICE_TIME\"\n  )\n\n  # Create empty CSV with correct header\n  df_in &lt;- tibble::tibble(\n    PATIENT_ID   = character(),\n    ARRIVAL_DATE = character(),\n    ARRIVAL_TIME = character(),\n    SERVICE_DATE = character(),\n    SERVICE_TIME = character()\n  )\n  csv_path &lt;- tempfile(fileext = \".csv\")\n  readr::write_csv(df_in, csv_path)\n\n  # Should succeed and return empty data frame\n  result &lt;- import_patient_data(csv_path)\n  expect_identical(nrow(result), 0L)\n  expect_identical(names(result), expected_cols)\n})\n\n\n\n\n5. Test all equivalent input forms\nIf the function promises to accept multiple equivalent input types, verify they really are equivalent.\n\nWith import_patient_data(), we expect a str or Path object to both succeed and return the same DataFrame.\n\ndef test_import_path_types(tmp_path):\n    \"\"\"str and Path inputs should behave identically.\"\"\"\n    # Create temporary CSV file\n    expected_cols = [\n        \"PATIENT_ID\",\n        \"ARRIVAL_DATE\", \"ARRIVAL_TIME\",\n        \"SERVICE_DATE\", \"SERVICE_TIME\",\n    ]\n    df_in = pd.DataFrame(\n        [[\"p1\", \"2024-01-01\", \"08:00\", \"2024-01-01\", \"09:00\"]],\n        columns=expected_cols,\n    )\n    csv_path = tmp_path / \"patients.csv\"\n    df_in.to_csv(csv_path, index=False)\n\n    # Run function with str or Path inputs\n    df_str = import_patient_data(str(csv_path))\n    df_path = import_patient_data(csv_path)\n\n    # Check that results are the same\n    pd.testing.assert_frame_equal(df_str, df_path)\n\n\n\nIn this case, import_patient_data() just accepts a character string for path, so there is nothing to test.",
    "crumbs": [
      "Types of test",
      "Unit tests"
    ]
  },
  {
    "objectID": "pages/unit_tests.html#running-our-example-tests",
    "href": "pages/unit_tests.html#running-our-example-tests",
    "title": "Unit tests",
    "section": "Running our example tests",
    "text": "Running our example tests\n\n\n\n\n\n\nNoteTest output\n\n\n\n\n\n\n============================= test session starts ==============================\nplatform linux -- Python 3.12.12, pytest-9.0.2, pluggy-1.6.0\nrootdir: /__w/hdruk_tests/hdruk_tests/examples/python_package\nconfigfile: pyproject.toml\nplugins: cov-7.0.0\ncollected 6 items\n\n../examples/python_package/tests/test_unit.py ......                     [100%]\n\n============================== 6 passed in 0.95s ===============================\n&lt;ExitCode.OK: 0&gt;\n\n\n\n\n\n\n\n‚ïê‚ïê Testing test_unit.R ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 0 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 3 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 4 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 5 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 6 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 7 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 8 ] Done!",
    "crumbs": [
      "Types of test",
      "Unit tests"
    ]
  },
  {
    "objectID": "pages/unit_tests.html#when-to-stop-writing-tests",
    "href": "pages/unit_tests.html#when-to-stop-writing-tests",
    "title": "Unit tests",
    "section": "When to stop writing tests",
    "text": "When to stop writing tests\nYou cannot test everything. You‚Äôve written enough tests when:\n\nEvery promise in the docstring is tested.\nEvery important code branch (like error handling) is tested.\n\nWrite additional tests based on real needs (e.g., bug reports, tricky edge cases in your context), and not by trying to anticipate every theoretical failure.\n\nIn real research projects, you won‚Äôt unit test every single function or every possible case. The aim is not perfection, but reasonable confidence in the most important behaviours of your code.",
    "crumbs": [
      "Types of test",
      "Unit tests"
    ]
  },
  {
    "objectID": "pages/run_tests.html",
    "href": "pages/run_tests.html",
    "title": "How to run tests",
    "section": "",
    "text": "Choose your language:¬†¬†\n    Python\n    R",
    "crumbs": [
      "Introduction to writing and running tests",
      "How to run tests"
    ]
  },
  {
    "objectID": "pages/run_tests.html#required-structure-for-running-tests-from-the-command-line",
    "href": "pages/run_tests.html#required-structure-for-running-tests-from-the-command-line",
    "title": "How to run tests",
    "section": "Required structure for running tests from the command line",
    "text": "Required structure for running tests from the command line\nThis approach works when you have:\n\n1. Test files that import your analysis code\nYour test files need to import the functions, classes, or modules you want to test. This can be done in several ways depending on your project structure:\n# If your code is packaged\nfrom waitingtimes.patient_analysis import summary_stats\n\n# If your code is in a local file in the same directory\nfrom patient_analysis import summary_stats\n\n# If your code is in a subdirectory\nfrom src.analysis import summary_stats\n\n\n2. Test files following the test_ naming convention\nPytest automatically discovers and runs tests when your test files start with test_. It is also typical (but not mandatory) to store test files in a folder called tests/. For example:\ntests/\n‚îú‚îÄ‚îÄ test_back.py\n‚îú‚îÄ‚îÄ test_functional.py\n‚îî‚îÄ‚îÄ test_unit.py",
    "crumbs": [
      "Introduction to writing and running tests",
      "How to run tests"
    ]
  },
  {
    "objectID": "pages/run_tests.html#common-pytest-commands",
    "href": "pages/run_tests.html#common-pytest-commands",
    "title": "How to run tests",
    "section": "Common pytest commands",
    "text": "Common pytest commands\nOnce your project follows these conventions, you can run tests from a terminal, PowerShell, or command prompt (depending on your OS). Common commands include:\n# Run all tests in the current directory and subdirectories\npytest\n\n# Run all tests in the tests/ directory\npytest tests/\n\n# Run tests from a specific file\npytest tests/test_filename.py\n\n# Run a specific test function\npytest tests/test_filename.py::test_function_name",
    "crumbs": [
      "Introduction to writing and running tests",
      "How to run tests"
    ]
  },
  {
    "objectID": "pages/run_tests.html#example-package-structure",
    "href": "pages/run_tests.html#example-package-structure",
    "title": "How to run tests",
    "section": "Example: package structure",
    "text": "Example: package structure\n\nWhen your code is structured as a package, your project might look like:\nproject/\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îî‚îÄ‚îÄ patient_data.csv\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ waitingtimes/\n‚îÇ       ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ       ‚îî‚îÄ‚îÄ patient_analysis.py\n‚îî‚îÄ‚îÄ tests/\n    ‚îú‚îÄ‚îÄ test_intro_simple.py\n    ‚îî‚îÄ‚îÄ ...\nThe video below demonstrates running tests on a package structured this way. In the video we:\n\nUse tree to display the file structure (matches that shown above).\nActivate our environment: conda activate hdruk_tests (which includes pytest).\nInstall our local package (if not already installed): pip install -e ..\nVerify the installation with conda list, confirming our waitingtimes package appears.\nShow you the test file (nano tests/test_intro_simple.py) (matches the one from the previous page on writing basic tests).\nRun the test with pytest tests/test_intro_simple.py - and it passes! üéâ\n\n\n\n\nIn an R package, tests will be automatically discovered and run if they follow the conventions of being:\n\nStored within tests/testthat/.\nIn R files starting with test_ or test-.\n\nFor example, your project might look like:\nproject/\n‚îú‚îÄ‚îÄ inst/\n‚îÇ   ‚îî‚îÄ‚îÄ extdata/\n‚îÇ       ‚îî‚îÄ‚îÄ patient_data.csv\n‚îú‚îÄ‚îÄ man/\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ R/\n‚îÇ   ‚îî‚îÄ‚îÄ patient_analysis.R\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îî‚îÄ‚îÄ testthat/\n‚îÇ       ‚îú‚îÄ‚îÄ test_intro_simple.R\n‚îÇ       ‚îî‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ .Rbuildignore\n‚îú‚îÄ‚îÄ DESCRIPTION\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ LICENSE.md\n‚îú‚îÄ‚îÄ NAMESPACE\n‚îî‚îÄ‚îÄ README.md\nYou can either run the tests with devtools or testthat - common commands include:\n# Run all tests in the package\ndevtools::test()\n\n# Run all tests in the named file\ntestthat::test_file(\"tests/testthat/test-patient_analysis.R\")\n\n# Run all tests in that directory\ntestthat::test_dir(\"tests/testthat\")\nIn the video below, we have an example of a research project structured as an our R project.\n\nWe highlight that the renv is active (Project '~/Documents/stars/hdruk_tests' loaded. [renv 1.1.5]).\nWe show the test file we will be running (test_intro_simple.R).\nWe run devtools::test() and can see that all tests pass.\n\n\n\nIf you‚Äôre unfamiliar with how to set up a package, check out our tutorial on packaging your research project.",
    "crumbs": [
      "Introduction to writing and running tests",
      "How to run tests"
    ]
  },
  {
    "objectID": "pages/run_tests.html#example-non-package-structure",
    "href": "pages/run_tests.html#example-non-package-structure",
    "title": "How to run tests",
    "section": "Example: Non-package structure",
    "text": "Example: Non-package structure\n\nYou don‚Äôt need a full package to use pytest. When your code exists in .py files but isn‚Äôt packaged, you can still run tests from the terminal. Your project might, for example, look like:\nproject/\n‚îú‚îÄ‚îÄ patient_analysis.py\n‚îî‚îÄ‚îÄ test_intro_simple.py\nIn the video below, we:\n\nUse tree to display the file structure - there is just a .py file with the summary_stats() function and test_intro_simple.py file alongside it.\nView the test file with nano test_intro_simple.py, showing it imports locally since files are in the same folder.\nRun the test with pytest, since the file follows the test_ naming pattern.\n\n\n\n\n\nYou don‚Äôt need a full package to use testthat. When your code exists in .R files but isn‚Äôt packaged, you can still run tests from the R console. You project might, for example, look like:\nproject/\n‚îú‚îÄ‚îÄ patient_analysis.R\n‚îî‚îÄ‚îÄ test_intro_simple.R\nAs your test follows of the convention of starting test_, it can still be detected by testthat and run by testthat:test_dir() or testthat::test_file().\n\nNote: You cannot use devtools::test() with this approach.\n\nIn the video below, we:\n\nOpen the test_intro_simple.R file, showing how it imports the local function.\nRun testthat::test_dir(\".\"), which finds and runs our test.",
    "crumbs": [
      "Introduction to writing and running tests",
      "How to run tests"
    ]
  },
  {
    "objectID": "pages/run_tests.html#alternative-options",
    "href": "pages/run_tests.html#alternative-options",
    "title": "How to run tests",
    "section": "Alternative options",
    "text": "Alternative options\n\nWhile running tests from the command line is the recommended approach, there are alternative tools for specific use cases.\n\ntestbook\ntestbook allows you to test code within Jupyter notebooks:\n# Install: pip install testbook\nfrom testbook import testbook\n\n@testbook('my_notebook.ipynb', execute=True)\ndef test_notebook_function(tb):\n    func = tb.ref(\"add\")\n    assert func(2, 3) == 5\n\n\nipytest\nipytest lets you run pytest directly inside Jupyter notebook cells:\n# Install: pip install ipytest\nimport ipytest\nipytest.autoconfig()\n\n# In another cell\ndef add(a, b):\n    return a + b\n\n# In a test cell\n%%ipytest\ndef test_add():\n    assert add(2, 3) == 5\n\n\n\nWhile running tests from the R console is the recommended approach, it is also possible to put tests directly in the same .R file as your code and just run the script.\nFor example:\n\n# add.R\n\nadd &lt;- function(a, b) {\n  a + b\n}\n\nlibrary(testthat)\n\ntest_that(\"add works correctly\", {\n  expect_equal(add(2, 3), 5)\n  expect_equal(add(-1, 1), 0)\n})\n\nTest passed with 2 successes üéä.\n\n\n\n\nWhy command line testing is better\nRunning tests from the terminal with code organised into separate files is the preferred approach because:\n\nTest code is separate from analysis code, making both easier to understand and maintain.\nTests can be run automatically in continuous integration (CI) pipelines, before commits, or on schedule.\nAs your project grows, you can easily add more test files and organise them logically without cluttering notebooks or scripts.",
    "crumbs": [
      "Introduction to writing and running tests",
      "How to run tests"
    ]
  },
  {
    "objectID": "pages/example_repositories.html",
    "href": "pages/example_repositories.html",
    "title": "Example repositories",
    "section": "",
    "text": "This page signposts a handful of healthcare projects that include automated tests across different focuses, data types, and analysis techniques.\nThey are provided to give you inspiration and ideas for what you might want to test in your research projects.",
    "crumbs": [
      "Example repositories"
    ]
  },
  {
    "objectID": "pages/example_repositories.html#python-examples",
    "href": "pages/example_repositories.html#python-examples",
    "title": "Example repositories",
    "section": "Python examples",
    "text": "Python examples\n\n  pydesrap_mms    pydesrap_stroke \nAnalysis: SimPy discrete-event simulation models (nurse visit simulation and stroke capacity planning).\nTests: pytest\nAuthors: Amy Heather and Tom Monks\n\n\n  nhp_model \nAnalysis: Demand model for the New Hospital Programme (NHP).\nTests: pytest\nAuthors: The Strategy Unit",
    "crumbs": [
      "Example repositories"
    ]
  },
  {
    "objectID": "pages/example_repositories.html#r-examples",
    "href": "pages/example_repositories.html#r-examples",
    "title": "Example repositories",
    "section": "R examples",
    "text": "R examples\n\n  rdesrap_mms    rdesrap_stroke \nAnalysis: Simmer discrete-event simulation models (nurse visit simulation and stroke capacity planning).\nTests: testthat\nAuthors: Amy Heather and Tom Monks\n\n\n  experiencesdashboard \nAnalysis: Shiny dashboard visualising and analysing patient experience data from the NHS England Friends and Family Test.\nTests: testthat\nAuthors: The Strategy Unit\n\n\n  NHSRwaitinglist \nAnalysis: Implements the waiting list management approach described in Fong et al.\nTests: testthat\nAuthors: Methodology presented by Neil Walton, and package contributed to by members of the NHS-R Community.\n\n\n  opencodecounts \nAnalysis: Yearly summaries of clinical code usage in England.\nTests: testthat\nAuthors: Arina Anna Tamborska, Rose Higgins, Yamina Boukari, Viveck Kingsley, Lola Ojedele, Kunle Oreagba, Jon Massey, Andrea Schaffer, Amelia Green, William Hulme, Brian MacKenna, Helen J Curtis, Louis Fisher, Milan Wiedemann.",
    "crumbs": [
      "Example repositories"
    ]
  },
  {
    "objectID": "pages/why_test.html",
    "href": "pages/why_test.html",
    "title": "When and why to run tests?",
    "section": "",
    "text": "Testing is not unique to software development. It is an essential part of research. Most researchers already test their work informally: they interrogate raw data for issues, scan tables, inspect plots, and check that values fall within plausible ranges.\nFormal testing simply records those expectations in code so they can be run consistently over time, catching issues as they arise rather than months later. Tests ask questions like:\nThis is the same logic you apply when checking results manually, just automated so you can repeat it systematically.",
    "crumbs": [
      "When and why to run tests?"
    ]
  },
  {
    "objectID": "pages/why_test.html#why-tests-matter-in-research-projects",
    "href": "pages/why_test.html#why-tests-matter-in-research-projects",
    "title": "When and why to run tests?",
    "section": "Why tests matter in research projects",
    "text": "Why tests matter in research projects\nWriting tests pays off because research code and data evolve. When you change your code, introduce new features, or update your data, you risk breaking things that used to work. Tests catch these problems early.\nThey help you verify that:\n\nYour logic and processes are working as you believe they are.\nRecent changes don‚Äôt break existing code.\nUpdated datasets are still valid and free from unexpected anomalies.\nYour results remain consistent and reproducible.\n\nOver time, when multiple people work on a project, or when you revisit analyses months later, tests become invaluable. They ensure that re-running your work produces the same results as it did previously, and that anyone depending on your outputs can trust them.\nUltimately, tests are not about best practice. They are about verifying your analysis, so you can trust that your results mean what you think they mean.",
    "crumbs": [
      "When and why to run tests?"
    ]
  },
  {
    "objectID": "pages/why_test.html#when-to-write-tests",
    "href": "pages/why_test.html#when-to-write-tests",
    "title": "When and why to run tests?",
    "section": "When to write tests",
    "text": "When to write tests\nYou don‚Äôt need a finished analysis to start testing. Even a single function or data processing step is worth testing. As you build your analysis, follow a simple pattern:\n\nWrite a piece of code (a function, data processing step, calculation).\nInspect the output to check it looks right.\nTurn that check into a test so you can run it automatically.\nIf you spot a bug, write a test that catches it.\nFix the bug, knowing your test will catch it if it happens again.\n\nThis way, testing becomes part of your natural workflow rather than a separate task.\nAvoiding leaving all testing until the end of your analysis. Problems discovered late are much harder to fix, and you may not fully understand their impact on your results. Testing as you go makes the process feel natural and keeps issues manageable.\n\n\n\n\n\n\nNoteTest-driven development\n\n\n\nSome software engineers write tests before code - this is called ‚Äútest-driven development‚Äù. This can feel less relevant in exploratory academic research, but if it suits your workflow, it is worth trying.",
    "crumbs": [
      "When and why to run tests?"
    ]
  },
  {
    "objectID": "pages/why_test.html#when-to-run-tests",
    "href": "pages/why_test.html#when-to-run-tests",
    "title": "When and why to run tests?",
    "section": "When to run tests",
    "text": "When to run tests\nRun tests regularly after any code or data changes, as catching errors earlier makes them easier to fix.\nThis practice of re-running tests is called regression testing, and it ensures recent changes haven‚Äôt introduced errors.",
    "crumbs": [
      "When and why to run tests?"
    ]
  },
  {
    "objectID": "pages/write_basic_test.html",
    "href": "pages/write_basic_test.html",
    "title": "How to write a basic test",
    "section": "",
    "text": "Choose your language:¬†¬†\n    Python\n    R",
    "crumbs": [
      "Introduction to writing and running tests",
      "How to write a basic test"
    ]
  },
  {
    "objectID": "pages/write_basic_test.html#what-is-pytest",
    "href": "pages/write_basic_test.html#what-is-pytest",
    "title": "How to write a basic test",
    "section": "What is pytest?",
    "text": "What is pytest?\nPytest is a popular framework for testing in Python, widely used in software development and data science.\nYou should install the pytest package into your environment from either conda or PyPI:\nconda install pytest\n# or\npip install pytest",
    "crumbs": [
      "Introduction to writing and running tests",
      "How to write a basic test"
    ]
  },
  {
    "objectID": "pages/write_basic_test.html#what-a-pytest-test-looks-like",
    "href": "pages/write_basic_test.html#what-a-pytest-test-looks-like",
    "title": "How to write a basic test",
    "section": "What a pytest test looks like",
    "text": "What a pytest test looks like\nIn pytest, any function whose name starts with test_ is treated as a test. Inside the function you write one or more assert statements. If an assertion fails, pytest will return an error message explaining what went wrong.\nFor example:\n\nimport pytest\n\n\ndef test_example():\n    \"\"\"Simple test confirming 2 + 2 = 4\"\"\"\n    result = 2 + 2\n    assert result == 4",
    "crumbs": [
      "Introduction to writing and running tests",
      "How to write a basic test"
    ]
  },
  {
    "objectID": "pages/write_basic_test.html#testthat",
    "href": "pages/write_basic_test.html#testthat",
    "title": "How to write a basic test",
    "section": "testthat",
    "text": "testthat\ntestthat is a popular framework for testing in R, widely used in software development and data science.\nYou should install the testthat package into your environment from CRAN.\ninstall.packages(\"testthat\")\nIf you are structuring your research as a package, we suggest also installing usethis and running:\nusethis::use_testthat()",
    "crumbs": [
      "Introduction to writing and running tests",
      "How to write a basic test"
    ]
  },
  {
    "objectID": "pages/write_basic_test.html#what-a-testthat-test-looks-like",
    "href": "pages/write_basic_test.html#what-a-testthat-test-looks-like",
    "title": "How to write a basic test",
    "section": "What a testthat test looks like",
    "text": "What a testthat test looks like\nTests are created using test_that(). They are built around expectations like expect_true(), expect_false(), expect_equal(), expect_error(), and others (see package index for more). If an expectation fails, testthat will return an error message explaining what went wrong.\nFor example:\n\nlibrary(testthat)\n\n\ntest_that(\"2 add 2 equals 4\", {\n  result &lt;- 2L + 2L\n  expect_equal(result, 4L)\n})\n\nTest passed with 1 success ü•á.",
    "crumbs": [
      "Introduction to writing and running tests",
      "How to write a basic test"
    ]
  },
  {
    "objectID": "pages/write_basic_test.html#a-simple-test-for-summary_stats",
    "href": "pages/write_basic_test.html#a-simple-test-for-summary_stats",
    "title": "How to write a basic test",
    "section": "A simple test for summary_stats",
    "text": "A simple test for summary_stats\nHere is a minimal example using the summary_stats function from the case study.\n\n\n\n\n\n\n\nNoteView summary_stats()\n\n\n\n\n\n\ndef summary_stats(data):\n    \"\"\"\n    Calculate mean, standard deviation and 95% confidence interval (CI).\n\n    CI is calculated using the t-distribution, which is appropriate for\n    small samples and converges to the normal distribution as the sample\n    size increases.\n\n    Parameters\n    ----------\n    data : pandas.Series\n        Data to use in the calculation.\n\n    Returns\n    -------\n    dict[str, float]\n        A dictionary with keys `mean`, `std_dev`, `ci_lower` and `ci_upper`.\n        Each value is a float, or `numpy.nan` if it can't be computed.\n    \"\"\"\n    # Drop missing values\n    data = data.dropna()\n\n    # Find number of observations\n    count = len(data)\n\n    # If there are no observations, then set all to NaN\n    if count == 0:\n        mean, std_dev, ci_lower, ci_upper = np.nan, np.nan, np.nan, np.nan\n\n    # If there are 1 or 2 observations, can do mean but not other statistics\n    elif count &lt; 3:\n        mean = data.mean()\n        std_dev, ci_lower, ci_upper = np.nan, np.nan, np.nan\n\n    # With more than two observations, can calculate all...\n    else:\n        mean = data.mean()\n        std_dev = data.std()\n\n        # If there is no variation, then CI is equal to the mean\n        if np.var(data) == 0:\n            ci_lower, ci_upper = mean, mean\n        else:\n            # 95% CI based on the t-distribution\n            ci_lower, ci_upper = st.t.interval(\n                confidence=0.95,\n                df=count-1,\n                loc=mean,\n                scale=st.sem(data)\n            )\n\n    return {\n        \"mean\": mean,\n        \"std_dev\": std_dev,\n        \"ci_lower\": ci_lower,\n        \"ci_upper\": ci_upper\n    }\n\n\n\n\n\n\n\n\n\n\n\n\nNoteView summary_stats()\n\n\n\n\n\n\n#' Calculate mean, standard deviation and 95% confidence interval (CI).\n#'\n#' CI is calculated using the t-distribution, which is appropriate for\n#' small samples and converges to the normal distribution as the sample\n#' size increases.\n#'\n#' @param data Numeric vector of data to use in the calculation.\n#'\n#' @return A named list with elements `mean`, `std_dev`, `ci_lower` and \n#'   `ci_upper`. Each value is a numeric, or `NA` if it can't be computed.\n#'\n#' @export\nsummary_stats &lt;- function(data) {\n  tibble::tibble(value = data) |&gt;\n    dplyr::reframe(\n      n_complete = sum(!is.na(value)),\n      mean = mean(value, na.rm = TRUE),\n      std_dev = stats::sd(value, na.rm = TRUE),\n      ci_lower   = {\n        if (n_complete &lt; 2L) {\n          NA_real_\n        } else if (std_dev == 0 || is.na(std_dev)) {\n          mean       # CI collapses to mean when no variation\n        } else {\n          stats::t.test(value)$conf.int[1L]\n        }\n      },\n      ci_upper   = {\n        if (n_complete &lt; 2L) {\n          NA_real_\n        } else if (std_dev == 0 || is.na(std_dev)) {\n          mean       # CI collapses to mean when no variation\n        } else {\n          stats::t.test(value)$conf.int[2L]\n        }\n      }\n    ) |&gt;\n    dplyr::select(-n_complete) |&gt;\n    as.list()\n}\n\n\n\n\n\nFor a single value, the function should return that value as the mean and NaN for the other statistics, because there is not enough data to define a standard deviation or confidence interval.\n\n\nimport pandas as pd\nfrom waitingtimes.patient_analysis import summary_stats\n\n\ndef test_summary_stats_single_value():\n    \"\"\"Running summary_stats on a single value should only return mean.\"\"\"\n    data = pd.Series([10])\n    res = summary_stats(data)\n    assert res[\"mean\"] == 10\n    assert pd.isna(res[\"std_dev\"])\n    assert pd.isna(res[\"ci_lower\"])\n    assert pd.isna(res[\"ci_upper\"])\n\n\n\n\ntest_that(\"running summary_stats on a single value only returns the mean\", {\n  data &lt;- c(10)\n  res &lt;- summary_stats(data)\n\n  expect_identical(res$mean, 10)\n  expect_true(is.na(res$std_dev))\n  expect_true(is.na(res$ci_lower))\n  expect_true(is.na(res$ci_upper))\n})",
    "crumbs": [
      "Introduction to writing and running tests",
      "How to write a basic test"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Testing in Research Workflows",
    "section": "",
    "text": "Testing is an essential part of reproducible and reliable research. This practical course explains how to write and run tests, covering key ideas such as unit and integration testing, test coverage, and automated testing with GitHub Actions. Using hands-on examples in Python and R, you‚Äôll learn how to build tests and include them in your research workflow."
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "Testing in Research Workflows",
    "section": "",
    "text": "Testing is an essential part of reproducible and reliable research. This practical course explains how to write and run tests, covering key ideas such as unit and integration testing, test coverage, and automated testing with GitHub Actions. Using hands-on examples in Python and R, you‚Äôll learn how to build tests and include them in your research workflow."
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Testing in Research Workflows",
    "section": "Instructors",
    "text": "Instructors\n\n\n\n\n\n\n\n\n\nAmy Heather\nPostdoctoral Research Associate at the University of Exeter\n  ORCID    GitHub    LinkedIn \n\n\n\n\n\n\n\n\n\n\nTom Monks\nAssociate Professor of Health Data Science at the University of Exeter\n  ORCID    GitHub    LinkedIn"
  },
  {
    "objectID": "index.html#funding",
    "href": "index.html#funding",
    "title": "Testing in Research Workflows",
    "section": "Funding",
    "text": "Funding\nThis course was developed as part of the STARS project. STARS is supported by the Medical Research Council [grant number MR/Z503915/1]."
  },
  {
    "objectID": "examples/r_package/LICENSE.html",
    "href": "examples/r_package/LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2026 Amy Heather and Tom Monks\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing",
    "section": "",
    "text": "This file is for contributors. It describes how the hdruk_tests site is set-up and quality controlled.\n\n\nThe example code is contained in examples.\nWe want to be able to show individual functions without imports in the quarto website, so we have a script tools/extract_snippets.py which extracts each function without imports into individual .py files within pages/code/. This is run each time the site is built via Quarto‚Äôs pre-render hook.\nExample commands for the python package:\n\npip install -e examples/python_package\npytest examples/python_package\n\nExample commands for the R package (having first opened R console by running R - escaped with quit()):\n\ndevtools::document(\"examples/r_package\")\ndevtools::check(\"examples/r_package\")\nwithr::with_dir(\"examples/r_package\", {usethis::use_mit_license()})\ndevtools::test(\"examples/r_package\")\n\n\n\n\nThe site is hosted on GitHub pages and rendered via GitHub actions. It uses a Docker environment which has the Python and R requirements.\nIf you wish to render it locally, please refer to the environment.yaml and renv.lock files.\n\n\nWhen rendering a Quarto document containing executable python code with reticulate, you might encounter:\nError in `use_condaenv()`:\n! Unable to locate conda environment 'des-rap-book'.\nBacktrace:\n    ‚ñÜ\n 1. ‚îî‚îÄreticulate::use_condaenv(\"des-rap-book\", required = TRUE)\nThis can occur when multiple Conda or Mamba installations exist (e.g.¬†mambgaforge, miniconda3), or if R is using a different search path than the shell. By default, reticulate only looks in one location for environments, which can cause problems when environments are not where reticulate expects.\nTo fix this, set the RETICULATE_CONDA environment variable to the correct Conda or Mamba executable. To find the path to your executable, run:\nconda env list\nLook for your environment in the list. For example, if your environment is at /home/amy/mambaforge/envs/des-rap-book, then your Conda executable is likely at /home/amy/mambaforge/bin/conda.\nSet the environment variable like so:\nexport RETICULATE_CONDA=/home/amy/mambaforge/bin/conda\nNow render your book:\nquarto render\nTo avoid needing to set RETICULATE_CONDA every time you open a new terminal, add the export command to an .Renviron file in your project directory. This file is not tracked by Git, and is specific to you. Create the file and add:\nRETICULATE_CONDA=/home/amy/mambaforge/bin/conda"
  },
  {
    "objectID": "CONTRIBUTING.html#example-code",
    "href": "CONTRIBUTING.html#example-code",
    "title": "Contributing",
    "section": "",
    "text": "The example code is contained in examples.\nWe want to be able to show individual functions without imports in the quarto website, so we have a script tools/extract_snippets.py which extracts each function without imports into individual .py files within pages/code/. This is run each time the site is built via Quarto‚Äôs pre-render hook.\nExample commands for the python package:\n\npip install -e examples/python_package\npytest examples/python_package\n\nExample commands for the R package (having first opened R console by running R - escaped with quit()):\n\ndevtools::document(\"examples/r_package\")\ndevtools::check(\"examples/r_package\")\nwithr::with_dir(\"examples/r_package\", {usethis::use_mit_license()})\ndevtools::test(\"examples/r_package\")"
  },
  {
    "objectID": "CONTRIBUTING.html#rendering-the-quarto-site",
    "href": "CONTRIBUTING.html#rendering-the-quarto-site",
    "title": "Contributing",
    "section": "",
    "text": "The site is hosted on GitHub pages and rendered via GitHub actions. It uses a Docker environment which has the Python and R requirements.\nIf you wish to render it locally, please refer to the environment.yaml and renv.lock files.\n\n\nWhen rendering a Quarto document containing executable python code with reticulate, you might encounter:\nError in `use_condaenv()`:\n! Unable to locate conda environment 'des-rap-book'.\nBacktrace:\n    ‚ñÜ\n 1. ‚îî‚îÄreticulate::use_condaenv(\"des-rap-book\", required = TRUE)\nThis can occur when multiple Conda or Mamba installations exist (e.g.¬†mambgaforge, miniconda3), or if R is using a different search path than the shell. By default, reticulate only looks in one location for environments, which can cause problems when environments are not where reticulate expects.\nTo fix this, set the RETICULATE_CONDA environment variable to the correct Conda or Mamba executable. To find the path to your executable, run:\nconda env list\nLook for your environment in the list. For example, if your environment is at /home/amy/mambaforge/envs/des-rap-book, then your Conda executable is likely at /home/amy/mambaforge/bin/conda.\nSet the environment variable like so:\nexport RETICULATE_CONDA=/home/amy/mambaforge/bin/conda\nNow render your book:\nquarto render\nTo avoid needing to set RETICULATE_CONDA every time you open a new terminal, add the export command to an .Renviron file in your project directory. This file is not tracked by Git, and is specific to you. Create the file and add:\nRETICULATE_CONDA=/home/amy/mambaforge/bin/conda"
  },
  {
    "objectID": "pages/functional_tests.html",
    "href": "pages/functional_tests.html",
    "title": "Functional tests",
    "section": "",
    "text": "Choose your language:¬†¬†\n    Python\n    R",
    "crumbs": [
      "Types of test",
      "Functional tests"
    ]
  },
  {
    "objectID": "pages/functional_tests.html#what-is-a-functional-test",
    "href": "pages/functional_tests.html#what-is-a-functional-test",
    "title": "Functional tests",
    "section": "What is a functional test?",
    "text": "What is a functional test?\nFunctional tests are broader than unit tests. They focus on a whole feature or workflow, often involving several functions or classes working together. They check whether this end-to-end behaviour gives the correct outputs for given inputs, matching our requirements or expected results.",
    "crumbs": [
      "Types of test",
      "Functional tests"
    ]
  },
  {
    "objectID": "pages/functional_tests.html#example-waiting-times-case-study",
    "href": "pages/functional_tests.html#example-waiting-times-case-study",
    "title": "Functional tests",
    "section": "Example: waiting times case study",
    "text": "Example: waiting times case study\nWe will return to our waiting times case study which involved three functions:\n\nimport_patient_data() - imports raw patient data and checks that the required columns are present.\ncalculate_wait_times() - adds arrival and service datetimes, and waiting time in minutes.\nsummary_stats() - calculates mean, standard deviation and 95% confidence interval.\n\nUnlike unit tests, which check each function in isolation, functional tests run all three steps together and verify the end-to-end workflow produces correct results.\n\nWe will need the follow imports in our test script:\n\nimport numpy as np\nimport pandas as pd\nimport pytest\nfrom waitingtimes.patient_analysis import (\n    import_patient_data, calculate_wait_times, summary_stats\n)",
    "crumbs": [
      "Types of test",
      "Functional tests"
    ]
  },
  {
    "objectID": "pages/functional_tests.html#how-to-write-functional-tests",
    "href": "pages/functional_tests.html#how-to-write-functional-tests",
    "title": "Functional tests",
    "section": "How to write functional tests",
    "text": "How to write functional tests\n\n\n\n\n1. Identify the feature or workflow to test\nStart by choosing the complete feature or workflow you want to validate.\nIn our case study, we only have a simple three-step pipeline - but more complex projects may have multiple intersecting workflows you want to focus on.\n\n\n2. Define inputs and expected outputs\nThink about realistic scenarios that cover:\n\nSuccess cases: standard inputs where everything should work correctly.\nVariations: realistic variations in the inputs (e.g., different sample sizes, different distributions in the data input).\nEdge cases: unusual but plausible inputs (e.g., unusual sample sizes, boundary values).\nError cases: invalid inputs that should trigger errors.\n\n\n\n3. Write tests for these scenarios\n\nSuccess case: typical data\nThis test confirms the workflow succeeds with standard inputs and produces correct summary statistics.\n\n\ndef test_workflow_success(tmp_path):\n    \"\"\"Complete workflow should calculate correct wait statistics.\"\"\"\n\n    # Create test data with known values\n    test_data = pd.DataFrame({\n        \"PATIENT_ID\": [\"p1\", \"p2\", \"p3\"],\n        \"ARRIVAL_DATE\": [\"2024-01-01\", \"2024-01-01\", \"2024-01-02\"],\n        \"ARRIVAL_TIME\": [\"0800\", \"0930\", \"1015\"],\n        \"SERVICE_DATE\": [\"2024-01-01\", \"2024-01-01\", \"2024-01-02\"],\n        \"SERVICE_TIME\": [\"0830\", \"1000\", \"1045\"],\n    })\n\n    # Write test CSV\n    csv_path = tmp_path / \"patients.csv\"\n    test_data.to_csv(csv_path, index=False)\n\n    # Run complete workflow\n    df = import_patient_data(csv_path)\n    df = calculate_wait_times(df)\n    stats = summary_stats(df[\"waittime\"])\n\n    # Verify the workflow produces correct results\n    # Expected wait times: 30, 30, 30 minutes\n    assert stats[\"mean\"] == 30.0\n    assert stats[\"std_dev\"] == 0.0\n    assert stats[\"ci_lower\"] == 30.0\n    assert stats[\"ci_upper\"] == 30.0\n\n\n\n\ntest_that(\"complete workflow should calculate correct wait statistics\", {\n  # Complete workflow should calculate correct wait statistics.\n\n  # Create test data with known values\n  test_data &lt;- tibble::tibble(\n    PATIENT_ID   = c(\"p1\", \"p2\", \"p3\"),\n    ARRIVAL_DATE = c(\"2024-01-01\", \"2024-01-01\", \"2024-01-02\"),\n    ARRIVAL_TIME = c(\"0800\", \"0930\", \"1015\"),\n    SERVICE_DATE = c(\"2024-01-01\", \"2024-01-01\", \"2024-01-02\"),\n    SERVICE_TIME = c(\"0830\", \"1000\", \"1045\")\n  )\n\n  # Write test CSV\n  csv_path &lt;- tempfile(fileext = \".csv\")\n  readr::write_csv(test_data, csv_path)\n\n  # Run complete workflow\n  df &lt;- import_patient_data(csv_path)\n  df &lt;- calculate_wait_times(df)\n  stats &lt;- summary_stats(df$waittime)\n\n  # Verify the workflow produces correct results\n  # Expected wait times: 30, 30, 30 minutes\n  expect_identical(stats$mean, 30)\n  expect_identical(stats$std_dev, 0)\n  expect_identical(stats$ci_lower, 30)\n  expect_identical(stats$ci_upper, 30)\n})\n\n\n\n\nVariation: data with different distributions\nThis test confirms the workflow handles realistic variation in wait times.\n\n\ndef test_workflow_with_variation(tmp_path):\n    \"\"\"Workflow should correctly compute statistics for variable wait times.\"\"\"\n\n    # Create test data with known wait times: 15, 30, 45 minutes\n    test_data = pd.DataFrame({\n        \"PATIENT_ID\": [\"p1\", \"p2\", \"p3\"],\n        \"ARRIVAL_DATE\": [\"2024-01-01\", \"2024-01-01\", \"2024-01-01\"],\n        \"ARRIVAL_TIME\": [\"0800\", \"0900\", \"1000\"],\n        \"SERVICE_DATE\": [\"2024-01-01\", \"2024-01-01\", \"2024-01-01\"],\n        \"SERVICE_TIME\": [\"0815\", \"0930\", \"1045\"],\n    })\n\n    csv_path = tmp_path / \"patients.csv\"\n    test_data.to_csv(csv_path, index=False)\n\n    # Run complete workflow\n    df = import_patient_data(csv_path)\n    df = calculate_wait_times(df)\n    stats = summary_stats(df[\"waittime\"])\n\n    # Verify mean and standard deviation\n    assert stats[\"mean\"] == 30\n    assert np.isclose(stats[\"std_dev\"], 15)\n\n    # CI should be symmetric around mean for this small sample\n    assert stats[\"ci_lower\"] &lt; stats[\"mean\"] &lt; stats[\"ci_upper\"]\n\n\n\n\ntest_that(\"workflow should correctly compute statistics for variable wait times\", {\n  # Workflow should correctly compute statistics for variable wait times.\n\n  # Create test data with known wait times: 15, 30, 45 minutes\n  test_data &lt;- tibble::tibble(\n    PATIENT_ID   = c(\"p1\", \"p2\", \"p3\"),\n    ARRIVAL_DATE = c(\"2024-01-01\", \"2024-01-01\", \"2024-01-01\"),\n    ARRIVAL_TIME = c(\"0800\", \"0900\", \"1000\"),\n    SERVICE_DATE = c(\"2024-01-01\", \"2024-01-01\", \"2024-01-01\"),\n    SERVICE_TIME = c(\"0815\", \"0930\", \"1045\")\n  )\n\n  csv_path &lt;- tempfile(fileext = \".csv\")\n  readr::write_csv(test_data, csv_path)\n\n  # Run complete workflow\n  df &lt;- import_patient_data(csv_path)\n  df &lt;- calculate_wait_times(df)\n  stats &lt;- summary_stats(df$waittime)\n\n  # Verify mean and standard deviation\n  expect_identical(stats$mean, 30)\n  expect_equal(stats$std_dev, 15, tolerance = 1e-8)\n\n  # CI should be symmetric around mean for this small sample\n  expect_lt(stats$ci_lower, stats$mean)\n  expect_gt(stats$ci_upper, stats$mean)\n})\n\n\n\n\nError case: invalid input data\nThis test confirms the workflow fails appropriately when given invalid data.\n\n\ndef test_missing_date_error(tmp_path):\n    \"\"\"Workflow should raise error when dates are missing.\"\"\"\n\n    test_data = pd.DataFrame({\n        \"PATIENT_ID\": [\"p1\", \"p2\", \"p3\"],\n        \"ARRIVAL_DATE\": [\"2024-01-01\", \"2024-01-01\", \"2024-01-01\"],\n        \"ARRIVAL_TIME\": [\"0800\", \"0900\", \"1000\"],\n        \"SERVICE_DATE\": [\"2024-01-01\", pd.NaT, \"2024-01-01\"],\n        \"SERVICE_TIME\": [\"0830\", \"1000\", \"1045\"],\n    })\n\n    csv_path = tmp_path / \"patients.csv\"\n    test_data.to_csv(csv_path, index=False)\n\n    # Workflow should fail when calculating wait times with missing dates\n    df = import_patient_data(csv_path)\n    with pytest.raises(ValueError, match=\"time data\"):\n        df = calculate_wait_times(df)\n\n\n\n\ntest_that(\"workflow should raise error when dates are missing\", {\n  # Workflow should raise error when dates are missing.\n\n  test_data &lt;- tibble::tibble(\n    PATIENT_ID   = c(\"p1\", \"p2\", \"p3\"),\n    ARRIVAL_DATE = c(\"2024-01-01\", \"2024-01-01\", \"2024-01-01\"),\n    ARRIVAL_TIME = c(\"0800\", \"0900\", \"1000\"),\n    SERVICE_DATE = c(\"2024-01-01\", NA, \"2024-01-01\"),\n    SERVICE_TIME = c(\"0830\", \"1000\", \"1045\")\n  )\n\n  csv_path &lt;- tempfile(fileext = \".csv\")\n  readr::write_csv(test_data, csv_path)\n\n  # Workflow should fail when calculating wait times with missing dates\n  # Will also have warning from ymd_hm() about returning NA\n  df &lt;- import_patient_data(csv_path)\n  expect_warning(\n    expect_error(\n        calculate_wait_times(df),\n        regexp = \"Failed to parse arrival or service datetimes\"\n    ),\n    regexp = \"failed to parse\"\n  )\n})",
    "crumbs": [
      "Types of test",
      "Functional tests"
    ]
  },
  {
    "objectID": "pages/functional_tests.html#running-our-example-tests",
    "href": "pages/functional_tests.html#running-our-example-tests",
    "title": "Functional tests",
    "section": "Running our example tests",
    "text": "Running our example tests\n\n\n\n\n\n\nNoteTest output\n\n\n\n\n\n\n============================= test session starts ==============================\nplatform linux -- Python 3.12.12, pytest-9.0.2, pluggy-1.6.0\nrootdir: /__w/hdruk_tests/hdruk_tests/examples/python_package\nconfigfile: pyproject.toml\nplugins: cov-7.0.0\ncollected 3 items\n\n../examples/python_package/tests/test_functional.py ...                  [100%]\n\n============================== 3 passed in 0.99s ===============================\n&lt;ExitCode.OK: 0&gt;\n\n\n\n\n\n\n\n‚ïê‚ïê Testing test_functional.R ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 0 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 3 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 4 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 5 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 6 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 7 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 8 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 9 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 10 ] Done!",
    "crumbs": [
      "Types of test",
      "Functional tests"
    ]
  },
  {
    "objectID": "pages/functional_tests.html#when-to-stop-writing-tests",
    "href": "pages/functional_tests.html#when-to-stop-writing-tests",
    "title": "Functional tests",
    "section": "When to stop writing tests",
    "text": "When to stop writing tests\nYou cannot test everything. You‚Äôve written enough tests when:\n\nCritical workflows are covered with at least one success case.\nImport variations and edge cases are tested.\nKey error conditions are verified.\n\nFocus your testing effort on workflows that matter to your research and scenarios you‚Äôre likely to encounter in practice. You‚Äôre building confidence in your code, not trying to test every theoretical possibility.",
    "crumbs": [
      "Types of test",
      "Functional tests"
    ]
  },
  {
    "objectID": "pages/test_coverage.html",
    "href": "pages/test_coverage.html",
    "title": "Test coverage",
    "section": "",
    "text": "Choose your language:¬†¬†\n    Python\n    R\nCoverage refers to the percentage of your code that is executed when you run your tests. It can help you spot parts of your code that are not included in any tests.",
    "crumbs": [
      "Test coverage"
    ]
  },
  {
    "objectID": "pages/test_coverage.html#pytest-cov",
    "href": "pages/test_coverage.html#pytest-cov",
    "title": "Test coverage",
    "section": "pytest-cov",
    "text": "pytest-cov\nThe pytest-cov package can be used to run coverage calculations easily alongside pytest. You can install it from PyPI or conda:\npip install pytest-cov\nconda install pytest-cov\nTo calculate coverage, you can then simply run tests with the --cov flag:\npytest --cov",
    "crumbs": [
      "Test coverage"
    ]
  },
  {
    "objectID": "pages/test_coverage.html#running-pytest---cov-on-our-example",
    "href": "pages/test_coverage.html#running-pytest---cov-on-our-example",
    "title": "Test coverage",
    "section": "Running pytest --cov on our example",
    "text": "Running pytest --cov on our example\n\n\n\n\n\n\nNoteTest output\n\n\n\n\n\n============================= test session starts ==============================\nplatform linux -- Python 3.12.12, pytest-9.0.2, pluggy-1.6.0\nrootdir: /__w/hdruk_tests/hdruk_tests/examples/python_package\nconfigfile: pyproject.toml\nplugins: cov-7.0.0\ncollected 13 items\n\n../examples/python_package/tests/test_back.py .                          [  7%]\n../examples/python_package/tests/test_functional.py ...                  [ 30%]\n../examples/python_package/tests/test_intro_parametrised.py ..           [ 46%]\n../examples/python_package/tests/test_intro_simple.py .                  [ 53%]\n../examples/python_package/tests/test_unit.py ......                     [100%]\n\n================================ tests coverage ================================\n_______________ coverage: platform linux, python 3.12.12-final-0 _______________\n\nName                                                                      Stmts   Miss  Cover\n---------------------------------------------------------------------------------------------\n/workspace/examples/python_package/src/waitingtimes/__init__.py               1      0   100%\n/workspace/examples/python_package/src/waitingtimes/patient_analysis.py      30      1    97%\n---------------------------------------------------------------------------------------------\nTOTAL                                                                        31      1    97%\n============================== 13 passed in 1.91s ==============================\n&lt;ExitCode.OK: 0&gt;\n\n\n\n\nThe coverage results are under the banner:\n================================ tests coverage ================================\nYou can see we get nearly 100% coverage. But what does this actually mean?",
    "crumbs": [
      "Test coverage"
    ]
  },
  {
    "objectID": "pages/test_coverage.html#tools-for-calculating-test-coverage-in-r",
    "href": "pages/test_coverage.html#tools-for-calculating-test-coverage-in-r",
    "title": "Test coverage",
    "section": "Tools for calculating test coverage in R",
    "text": "Tools for calculating test coverage in R\nIf your research is structured as a package, then you can use devtools to calculate coverage:\ndevtools::test_coverage()\nIf not structured as a package, you can use covr‚Äôs file_coverage() function - for example:\ncovr::file_coverage(\n  source_files = c(\"patient_analysis.R\"),\n  test_files = c(\"test_unit.R\", \"test_functional.R\", \"test_back.R\")\n)",
    "crumbs": [
      "Test coverage"
    ]
  },
  {
    "objectID": "pages/test_coverage.html#calculating-coverage-for-our-example",
    "href": "pages/test_coverage.html#calculating-coverage-for-our-example",
    "title": "Test coverage",
    "section": "Calculating coverage for our example",
    "text": "Calculating coverage for our example\n\ndevtools::load_all(\"../examples/r_package\")\n\n\ndevtools::test_coverage(\"../examples/r_package\")\n\n‚Ñπ Computing test coverage for waitingtimes\n\n\nYou can see we get 100% coverage. But what does this actually mean?",
    "crumbs": [
      "Test coverage"
    ]
  },
  {
    "objectID": "pages/test_coverage.html#interpreting-coverage",
    "href": "pages/test_coverage.html#interpreting-coverage",
    "title": "Test coverage",
    "section": "Interpreting coverage",
    "text": "Interpreting coverage\nCoverage is telling you whether code was executed during testing - but not necessarily whether it has been tested well. A function could run as part of another test without its results or behaviour being properly checked by assertions.\n\nCoverage tells you what code ran, not whether it worked correctly.\n\nIt‚Äôs mostly useful for finding code that isn‚Äôt covered by tests at all. Having parts of your code with no/low coverage means:\n\nThey‚Äôre not imported or run by any tests.\nThey‚Äôre only used in rare branches or failure conditions.\nThey were added recently but have not yet been incorporated into tests.\n\nRather than try to achieve 100% coverage, you should aim to meaningfully test all your code: every important path, decision and behaviour should be tested at least once.",
    "crumbs": [
      "Test coverage"
    ]
  },
  {
    "objectID": "pages/back_tests.html",
    "href": "pages/back_tests.html",
    "title": "Back tests",
    "section": "",
    "text": "Choose your language:¬†¬†\n    Python\n    R",
    "crumbs": [
      "Types of test",
      "Back tests"
    ]
  },
  {
    "objectID": "pages/back_tests.html#what-is-a-back-test",
    "href": "pages/back_tests.html#what-is-a-back-test",
    "title": "Back tests",
    "section": "What is a back test?",
    "text": "What is a back test?\nA back test involves running your workflow on historical data and confirming that results are consistent over time.\nIt‚Äôs not focused on whether results are theoretically correct. It‚Äôs about consistency and reproducibility - confirming that code changes, environment updates, or data pipeline tweaks have not silently changed results.\nThis overlaps with functional testing, but is distinct, because functional tests ask ‚Äúis this output correct‚Äù whereas back tests ask ‚Äúis this output the same as before?‚Äù.",
    "crumbs": [
      "Types of test",
      "Back tests"
    ]
  },
  {
    "objectID": "pages/back_tests.html#example-waiting-times-case-study",
    "href": "pages/back_tests.html#example-waiting-times-case-study",
    "title": "Back tests",
    "section": "Example: waiting times case study",
    "text": "Example: waiting times case study\nWe will run back tests using the dataset we introduced for our waiting times case study.\n\nOn the test page, we need to import:\n\nfrom pathlib import Path\nimport numpy as np\nfrom waitingtimes.patient_analysis import (\n    import_patient_data, calculate_wait_times, summary_stats\n)",
    "crumbs": [
      "Types of test",
      "Back tests"
    ]
  },
  {
    "objectID": "pages/back_tests.html#back-test",
    "href": "pages/back_tests.html#back-test",
    "title": "Back tests",
    "section": "Back test",
    "text": "Back test\n\n\ndef test_reproduction():\n    \"\"\"Re-running on historical data should produce consistent results.\"\"\"\n    # Specify path to historical data\n    csv_path = Path(__file__).parent.joinpath(\"data/patient_data.csv\")\n\n    # Run functions\n    df = import_patient_data(csv_path)\n    df = calculate_wait_times(df)\n    stats = summary_stats(df[\"waittime\"])\n\n    # Verify the workflow produces consistent results\n    assert np.isclose(stats[\"mean\"], 4.1666, rtol=0.0001)\n    assert np.isclose(stats[\"std_dev\"], 2.7869, rtol=0.0001)\n    assert np.isclose(stats[\"ci_lower\"], 1.2420, rtol=0.0001)\n    assert np.isclose(stats[\"ci_upper\"], 7.0913, rtol=0.0001)\n\n\n\n\ntest_that(\"re-running on historical data produces consistent results\", {\n  # Re-running on historical data should produce consistent results.\n\n  # Specify path to historical data\n  csv_path &lt;- testthat::test_path(\"data\", \"patient_data.csv\")\n\n  # Run functions\n  df &lt;- import_patient_data(csv_path)\n  df &lt;- calculate_wait_times(df)\n  stats &lt;- summary_stats(df$waittime)\n\n  # Verify the workflow produces consistent results\n  expect_equal(stats$mean,     4.1666, tolerance = 1e-4)\n  expect_equal(stats$std_dev,  2.7869, tolerance = 1e-4)\n  expect_equal(stats$ci_lower, 1.2420, tolerance = 1e-4)\n  expect_equal(stats$ci_upper, 7.0913, tolerance = 1e-4)\n})",
    "crumbs": [
      "Types of test",
      "Back tests"
    ]
  },
  {
    "objectID": "pages/back_tests.html#running-our-example-test",
    "href": "pages/back_tests.html#running-our-example-test",
    "title": "Back tests",
    "section": "Running our example test",
    "text": "Running our example test\n\n\n\n\n\n\nNoteTest output\n\n\n\n\n\n\n============================= test session starts ==============================\nplatform linux -- Python 3.12.12, pytest-9.0.2, pluggy-1.6.0\nrootdir: /__w/hdruk_tests/hdruk_tests/examples/python_package\nconfigfile: pyproject.toml\nplugins: cov-7.0.0\ncollected 1 item\n\n../examples/python_package/tests/test_back.py .                          [100%]\n\n============================== 1 passed in 0.94s ===============================\n&lt;ExitCode.OK: 0&gt;\n\n\n\n\n\n\n\n‚ïê‚ïê Testing test_back.R ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 0 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 3 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 4 ] Done!",
    "crumbs": [
      "Types of test",
      "Back tests"
    ]
  },
  {
    "objectID": "pages/back_tests.html#when-should-you-update-your-back-tests",
    "href": "pages/back_tests.html#when-should-you-update-your-back-tests",
    "title": "Back tests",
    "section": "When should you update your back tests?",
    "text": "When should you update your back tests?\nErrors: If you identify an error in your pipeline, you first fix the code and then deliberately update the back test in isolation, so you know the only change in behaviour is the error fix and not something unintended elsewhere.\nChanges over time: As your research evolves, you may update the workflow (e.g., improve the wait time calculation method) or use more recent datasets. You can keep the old back test running alongside new ones - this verifies that changes to the workflow don‚Äôt accidentally alter results on historical data, while new back tests validate that updated methods work correctly on current data.",
    "crumbs": [
      "Types of test",
      "Back tests"
    ]
  },
  {
    "objectID": "pages/case_study.html",
    "href": "pages/case_study.html",
    "title": "Case study",
    "section": "",
    "text": "Choose your language:¬†¬†\n    Python\n    R\nOur tutorial uses a simple example of importing a small patient dataset and carrying out basic descriptive analysis.\nIn the example we will:",
    "crumbs": [
      "Case study"
    ]
  },
  {
    "objectID": "pages/case_study.html#packages",
    "href": "pages/case_study.html#packages",
    "title": "Case study",
    "section": "Packages",
    "text": "Packages\n\n\n\nimport json\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as st\n\npd.set_option(\"display.max_columns\", 8)\n\n\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(lubridate)",
    "crumbs": [
      "Case study"
    ]
  },
  {
    "objectID": "pages/case_study.html#import-patient-data",
    "href": "pages/case_study.html#import-patient-data",
    "title": "Case study",
    "section": "Import patient data",
    "text": "Import patient data\nOur dataset is a small synthetic set of patient-level event times that could come from a healthcare setting (e.g., emergency department, outpatient clinic, doctor‚Äôs appointments). Each row records when a patient arrived and when service began.\n\nOur function import_patient_data() reads the data from a CSV file and checks it contains the expected columns, returning a pandas DataFrame.\n\ndef import_patient_data(path):\n    \"\"\"\n    Import raw patient data and check that required columns are present.\n\n    Parameters\n    ----------\n    path : str or pathlib.Path\n        Path to the CSV file containing the patient data.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Dataframe containing the raw patient-level data.\n\n    Raises\n    ------\n    ValueError\n        If the CSV file does not contain exactly the expected columns\n        in the expected order.\n    \"\"\"\n    df = pd.read_csv(Path(path))\n\n    # Expected columns in the raw data (names and order must match)\n    expected = [\n        \"PATIENT_ID\",\n        \"ARRIVAL_DATE\", \"ARRIVAL_TIME\",\n        \"SERVICE_DATE\", \"SERVICE_TIME\"\n    ]\n    if list(df.columns) != expected:\n        raise ValueError(\n            f\"Unexpected columns: {list(df.columns)} (expected {expected})\"\n        )\n\n    return df\n\n\n\n\n#' Import raw patient data and check that required columns are present.\n#'\n#' Raises an error if the CSV file does not contain exactly the expected \n#' columns in the expected order.\n#'\n#' @param path Character string giving path to the CSV file containing the \n#'   patient data.\n#'\n#' @return A data frame containing the raw patient-level data.\n#'\n#' @export\nimport_patient_data &lt;- function(path) {\n  df &lt;- readr::read_csv(path, show_col_types = FALSE)\n\n  # Expected columns in the raw data (names and order must match)\n  expected &lt;- c(\n    \"PATIENT_ID\",\n    \"ARRIVAL_DATE\", \"ARRIVAL_TIME\",\n    \"SERVICE_DATE\", \"SERVICE_TIME\"\n  )\n  if (!identical(colnames(df), expected)) {\n    stop(\n      sprintf(\n        \"Unexpected columns: %s (expected %s)\",\n        paste(colnames(df), collapse = \", \"),\n        paste(expected, collapse = \", \")\n      )\n    )\n  }\n\n  return(df)\n}\n\n\nWe can run this function on our example dataset patient_data.csv.\nYou can download a copy of this data here:\n Download the example patient-level data \n\n\nraw_data = import_patient_data(\n   \"../examples/python_package/data/patient_data.csv\"\n)\nraw_data\n\n   PATIENT_ID ARRIVAL_DATE  ARRIVAL_TIME SERVICE_DATE  SERVICE_TIME\n0           1   2025-01-01             1   2025-01-01             7\n1           2   2025-01-01             2   2025-01-01             4\n2           3   2025-01-01             3   2025-01-01            10\n3           4   2025-01-01             7   2025-01-01            14\n4           5   2025-01-01            10   2025-01-01            12\n5           6   2025-01-01            10   2025-01-01            11\n\n\n\n\n\nraw_data &lt;- import_patient_data(\n  file.path(\"..\", \"examples\", \"r_package\", \"inst\", \"extdata\", \"patient_data.csv\")\n)\nraw_data\n\n# A tibble: 6 √ó 5\n  PATIENT_ID ARRIVAL_DATE ARRIVAL_TIME SERVICE_DATE SERVICE_TIME\n       &lt;dbl&gt; &lt;date&gt;       &lt;chr&gt;        &lt;date&gt;       &lt;chr&gt;       \n1          1 2025-01-01   0001         2025-01-01   0007        \n2          2 2025-01-01   0002         2025-01-01   0004        \n3          3 2025-01-01   0003         2025-01-01   0010        \n4          4 2025-01-01   0007         2025-01-01   0014        \n5          5 2025-01-01   0010         2025-01-01   0012        \n6          6 2025-01-01   0010         2025-01-01   0011",
    "crumbs": [
      "Case study"
    ]
  },
  {
    "objectID": "pages/case_study.html#calculate-waiting-times",
    "href": "pages/case_study.html#calculate-waiting-times",
    "title": "Case study",
    "section": "Calculate waiting times",
    "text": "Calculate waiting times\n\nNext, we convert the date and time fields into datetime columns and calculate each patient‚Äôs waiting time in minutes.\n\ndef calculate_wait_times(df):\n    \"\"\"\n    Add arrival/service datetimes and waiting time in minutes.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        Patient-level data containing `ARRIVAL_DATE`, `ARRIVAL_TIME`,\n        `SERVICE_DATE`, and `SERVICE_TIME` columns.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Copy of the input DataFrame with additional columns:\n        `arrival_datetime`, `service_datetime`, and `waittime`.\n    \"\"\"\n    df = df.copy()\n\n    # Combine date and time columns into datetime columns\n    for prefix in (\"ARRIVAL\", \"SERVICE\"):\n        df[f\"{prefix.lower()}_datetime\"] = pd.to_datetime(\n            df[f\"{prefix}_DATE\"].astype(str) +\n            \" \" +\n            df[f\"{prefix}_TIME\"].astype(str).str.zfill(4),\n            format=\"%Y-%m-%d %H%M\"\n        )\n\n    # Waiting time in minutes\n    df[\"waittime\"] = (\n        df[\"service_datetime\"] - df[\"arrival_datetime\"]\n    ) / pd.Timedelta(minutes=1)\n\n    return df\n\n\n\n\n#' Add arrival/service datetimes and waiting time in minutes.\n#'\n#' @param df Data frame with patient-level data containing `ARRIVAL_DATE`, \n#'   `ARRIVAL_TIME`, `SERVICE_DATE`, and `SERVICE_TIME` columns.\n#'\n#' @return A copy of the input data frame with additional columns:\n#'   `arrival_datetime`, `service_datetime`, and `waittime`.\n#'\n#' @export\ncalculate_wait_times &lt;- function(df) {\n  df &lt;- df |&gt;\n    dplyr::mutate(\n      arrival_datetime = lubridate::ymd_hm(\n        paste(\n          as.character(ARRIVAL_DATE),\n          sprintf(\"%04d\", as.integer(ARRIVAL_TIME))\n        )\n      ),\n      service_datetime = lubridate::ymd_hm(\n        paste(\n          as.character(SERVICE_DATE),\n          sprintf(\"%04d\", as.integer(SERVICE_TIME))\n        )\n      )\n    )\n\n  if (any(is.na(df$arrival_datetime) | is.na(df$service_datetime))) {\n    stop(\n      \"Failed to parse arrival or service datetimes; \",\n      \"check for missing or invalid dates/times.\"\n    )\n  }\n\n  df &lt;- df |&gt;\n    dplyr::mutate(\n      waittime = as.numeric(\n        difftime(service_datetime, arrival_datetime, units = \"mins\")\n      )\n    )\n\n  df\n}\n\n\nWe then apply this function to the raw data.\n\n\nprocessed_data = calculate_wait_times(raw_data)\nprocessed_data\n\n   PATIENT_ID ARRIVAL_DATE  ARRIVAL_TIME SERVICE_DATE  SERVICE_TIME  \\\n0           1   2025-01-01             1   2025-01-01             7   \n1           2   2025-01-01             2   2025-01-01             4   \n2           3   2025-01-01             3   2025-01-01            10   \n3           4   2025-01-01             7   2025-01-01            14   \n4           5   2025-01-01            10   2025-01-01            12   \n5           6   2025-01-01            10   2025-01-01            11   \n\n     arrival_datetime    service_datetime  waittime  \n0 2025-01-01 00:01:00 2025-01-01 00:07:00       6.0  \n1 2025-01-01 00:02:00 2025-01-01 00:04:00       2.0  \n2 2025-01-01 00:03:00 2025-01-01 00:10:00       7.0  \n3 2025-01-01 00:07:00 2025-01-01 00:14:00       7.0  \n4 2025-01-01 00:10:00 2025-01-01 00:12:00       2.0  \n5 2025-01-01 00:10:00 2025-01-01 00:11:00       1.0  \n\n\n\n\n\nprocessed_data &lt;- calculate_wait_times(raw_data)\nprocessed_data\n\n# A tibble: 6 √ó 8\n  PATIENT_ID ARRIVAL_DATE ARRIVAL_TIME SERVICE_DATE SERVICE_TIME\n       &lt;dbl&gt; &lt;date&gt;       &lt;chr&gt;        &lt;date&gt;       &lt;chr&gt;       \n1          1 2025-01-01   0001         2025-01-01   0007        \n2          2 2025-01-01   0002         2025-01-01   0004        \n3          3 2025-01-01   0003         2025-01-01   0010        \n4          4 2025-01-01   0007         2025-01-01   0014        \n5          5 2025-01-01   0010         2025-01-01   0012        \n6          6 2025-01-01   0010         2025-01-01   0011        \n# ‚Ñπ 3 more variables: arrival_datetime &lt;dttm&gt;, service_datetime &lt;dttm&gt;,\n#   waittime &lt;dbl&gt;",
    "crumbs": [
      "Case study"
    ]
  },
  {
    "objectID": "pages/case_study.html#calculate-summary-statistics",
    "href": "pages/case_study.html#calculate-summary-statistics",
    "title": "Case study",
    "section": "Calculate summary statistics",
    "text": "Calculate summary statistics\nFinally, we define a small utility function that calculates the mean, standard deviation, and a 95% confidence interval for a numeric series. The function handles small‚Äësample edge cases explicitly and uses the t‚Äëdistribution, which is appropriate when the sample size is modest.\n\n\ndef summary_stats(data):\n    \"\"\"\n    Calculate mean, standard deviation and 95% confidence interval (CI).\n\n    CI is calculated using the t-distribution, which is appropriate for\n    small samples and converges to the normal distribution as the sample\n    size increases.\n\n    Parameters\n    ----------\n    data : pandas.Series\n        Data to use in the calculation.\n\n    Returns\n    -------\n    dict[str, float]\n        A dictionary with keys `mean`, `std_dev`, `ci_lower` and `ci_upper`.\n        Each value is a float, or `numpy.nan` if it can't be computed.\n    \"\"\"\n    # Drop missing values\n    data = data.dropna()\n\n    # Find number of observations\n    count = len(data)\n\n    # If there are no observations, then set all to NaN\n    if count == 0:\n        mean, std_dev, ci_lower, ci_upper = np.nan, np.nan, np.nan, np.nan\n\n    # If there are 1 or 2 observations, can do mean but not other statistics\n    elif count &lt; 3:\n        mean = data.mean()\n        std_dev, ci_lower, ci_upper = np.nan, np.nan, np.nan\n\n    # With more than two observations, can calculate all...\n    else:\n        mean = data.mean()\n        std_dev = data.std()\n\n        # If there is no variation, then CI is equal to the mean\n        if np.var(data) == 0:\n            ci_lower, ci_upper = mean, mean\n        else:\n            # 95% CI based on the t-distribution\n            ci_lower, ci_upper = st.t.interval(\n                confidence=0.95,\n                df=count-1,\n                loc=mean,\n                scale=st.sem(data)\n            )\n\n    return {\n        \"mean\": mean,\n        \"std_dev\": std_dev,\n        \"ci_lower\": ci_lower,\n        \"ci_upper\": ci_upper\n    }\n\n\n\n\n#' Calculate mean, standard deviation and 95% confidence interval (CI).\n#'\n#' CI is calculated using the t-distribution, which is appropriate for\n#' small samples and converges to the normal distribution as the sample\n#' size increases.\n#'\n#' @param data Numeric vector of data to use in the calculation.\n#'\n#' @return A named list with elements `mean`, `std_dev`, `ci_lower` and \n#'   `ci_upper`. Each value is a numeric, or `NA` if it can't be computed.\n#'\n#' @export\nsummary_stats &lt;- function(data) {\n  tibble::tibble(value = data) |&gt;\n    dplyr::reframe(\n      n_complete = sum(!is.na(value)),\n      mean = mean(value, na.rm = TRUE),\n      std_dev = stats::sd(value, na.rm = TRUE),\n      ci_lower   = {\n        if (n_complete &lt; 2L) {\n          NA_real_\n        } else if (std_dev == 0 || is.na(std_dev)) {\n          mean       # CI collapses to mean when no variation\n        } else {\n          stats::t.test(value)$conf.int[1L]\n        }\n      },\n      ci_upper   = {\n        if (n_complete &lt; 2L) {\n          NA_real_\n        } else if (std_dev == 0 || is.na(std_dev)) {\n          mean       # CI collapses to mean when no variation\n        } else {\n          stats::t.test(value)$conf.int[2L]\n        }\n      }\n    ) |&gt;\n    dplyr::select(-n_complete) |&gt;\n    as.list()\n}\n\n\nWe apply summary_stats() to the waiting times.\n\n\nresults = summary_stats(processed_data[\"waittime\"])\n\n# Format dictionary for display\nformatted_results = json.dumps(results, indent=4)\nprint(formatted_results)\n\n{\n    \"mean\": 4.166666666666667,\n    \"std_dev\": 2.786873995477131,\n    \"ci_lower\": 1.2420217719136457,\n    \"ci_upper\": 7.091311561419689\n}\n\n\n\n\n\nresults &lt;- summary_stats(processed_data[[\"waittime\"]])\nresults\n\n$mean\n[1] 4.166667\n\n$std_dev\n[1] 2.786874\n\n$ci_lower\n[1] 1.242022\n\n$ci_upper\n[1] 7.091312",
    "crumbs": [
      "Case study"
    ]
  },
  {
    "objectID": "pages/break.html",
    "href": "pages/break.html",
    "title": "What was the point? Let‚Äôs break it and see!",
    "section": "",
    "text": "Choose your language:¬†¬†\n    Python\n    R\nWe have built a suite of unit, functional and back tests. These took time and effort to write. It is natural to ask: was it worth it?\nWe all know that tests ‚Äúgive confidence‚Äù and ‚Äúcheck things‚Äù, but that can feel abstract and slightly theoretical. To make it concrete, this page deliberately breaks the code and shows how the tests help.",
    "crumbs": [
      "What was the point? Let's break it and see!"
    ]
  },
  {
    "objectID": "pages/break.html#the-innocent-change-minutes-to-hours",
    "href": "pages/break.html#the-innocent-change-minutes-to-hours",
    "title": "What was the point? Let‚Äôs break it and see!",
    "section": "The ‚Äúinnocent change‚Äù: minutes to hours",
    "text": "The ‚Äúinnocent change‚Äù: minutes to hours\nSuppose someone in the team decides that using hours instead of minutes will be more convenient when summarising waiting times. They find this line in the processing code:\n\ndf[\"waittime\"] = (\n    df[\"service_datetime\"] - df[\"arrival_datetime\"]\n) / pd.Timedelta(minutes=1)\nThey change it to use hours instead:\ndf[\"waittime\"] = (\n    df[\"service_datetime\"] - df[\"arrival_datetime\"]\n) / pd.Timedelta(hours=1)\n\n\ndf &lt;- df |&gt;\ndplyr::mutate(\n    waittime = as.numeric(\n    difftime(service_datetime, arrival_datetime, units = \"mins\")\n    )\n)\nThey change it to use hours instead:\ndf &lt;- df |&gt;\ndplyr::mutate(\n    waittime = as.numeric(\n    difftime(service_datetime, arrival_datetime, units = \"hours\")\n    )\n)\n\nNothing else changes: same column name, same workflow, same analysis scripts.",
    "crumbs": [
      "What was the point? Let's break it and see!"
    ]
  },
  {
    "objectID": "pages/break.html#what-happens-next",
    "href": "pages/break.html#what-happens-next",
    "title": "What was the point? Let‚Äôs break it and see!",
    "section": "What happens next?",
    "text": "What happens next?\nLater, someone else on the team runs the analysis in exactly the same way as before. They are expecting average waits of around 30 minutes, because that is what the service normally sees.\nInstead, they now see an average of 0.5 in the output.\n\nThey might not immediately realise that 0.5 means 0.5 hours.\nThey might start looking for some other explanation: ‚ÄúHas demand changed?‚Äù ‚ÄúIs the data different?‚Äù‚Äù ‚ÄúDid we filter a different subset?‚Äù‚Äù\nThey might even present the wrong numbers.\n\nThe code is still ‚Äúworking‚Äù in the sense that it runs successfully and produces numbers, but those numbers are now in a different unit from before. The change in meaning is silent and potentially easy to miss.",
    "crumbs": [
      "What was the point? Let's break it and see!"
    ]
  },
  {
    "objectID": "pages/break.html#how-the-tests-can-help",
    "href": "pages/break.html#how-the-tests-can-help",
    "title": "What was the point? Let‚Äôs break it and see!",
    "section": "How the tests can help",
    "text": "How the tests can help\nThis is where tests come in handy! After the change to hours, the functional and back tests will both fail.\nThese failures tell the team something important: the problem is in the code, not in the new data or some hidden change in demand.\nWithout these tests, this kind of ‚Äúinnocent‚Äù change could easily slip through and only be discovered much later.",
    "crumbs": [
      "What was the point? Let's break it and see!"
    ]
  },
  {
    "objectID": "pages/github_actions.html",
    "href": "pages/github_actions.html",
    "title": "Running tests via GitHub actions",
    "section": "",
    "text": "Choose your language:¬†¬†\n    Python\n    R\nAs mentioned on the page When and why to run tests?, you should run tests regularly after any code or data changes, as catching errors earlier makes them easier to fix. This practice of re-running tests is called regression testing, and it ensures recent changes haven‚Äôt introduced errors.\nGitHub Actions can be a great tool to support this.",
    "crumbs": [
      "Running tests via GitHub actions"
    ]
  },
  {
    "objectID": "pages/github_actions.html#github-actions",
    "href": "pages/github_actions.html#github-actions",
    "title": "Running tests via GitHub actions",
    "section": "GitHub Actions",
    "text": "GitHub Actions\nGitHub is widely used for hosting research code and managing version control. We have a tutorial on setting up a repository if you are new to GitHUb.\nGitHub Actions is a built-in automation system that runs workflows directly in your repository. You can access it from the Actions tab on your GitHub repository page:\n\nWorkflows are defined using YAML files stored in .github/workflows/ in your repository. Each workflow can be triggered by one or more events, with common triggers including:\n\npush: run tests on every push to a branch.\npush: branches: [\"main\"]: run tests on every push to the main branch.\npull_request: run tests when a pull request is opened or updated.\nworkflow_dispatch: allows manual runs from the ‚ÄúActions‚Äù tab.",
    "crumbs": [
      "Running tests via GitHub actions"
    ]
  },
  {
    "objectID": "pages/github_actions.html#workflow-to-run-tests",
    "href": "pages/github_actions.html#workflow-to-run-tests",
    "title": "Running tests via GitHub actions",
    "section": "Workflow to run tests",
    "text": "Workflow to run tests\nThis workflow will run the tests from our case study via GitHub actions. We explain it step-by-step below.\n\n\nname: python_tests\nrun-name: Run python tests\n\non:\n  push:\n    branches: [main]\n  workflow_dispatch:\n\njobs:\n  tests:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        include:\n          - os: ubuntu-latest\n            python-version: '3.11'\n          - os: ubuntu-latest\n            python-version: '3.12'\n          - os: windows-latest\n            python-version: '3.12'\n          - os: macos-latest\n            python-version: '3.12'\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Install python and dependencies\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n          cache: 'pip'\n      \n      - name: Install requirements (Windows)\n        if: runner.os == 'Windows'\n        run: python -m pip install -r requirements-test.txt\n      \n      - name: Install requirements (Unix)\n        if: runner.os != 'Windows'\n        run: pip install -r requirements-test.txt\n\n      - name: Run tests\n        run: pytest examples/python_package\n\n\nExplaining the workflow\nname: python_tests\nrun-name: Run python tests\nThe beginning of the YAML sets the workflow‚Äôs name and how it appears in the Actions tab.\n\nname is the internal name of the workflow file.\nrun-name is what is displayed when a run appears in the Actions history.\n\non:\n  push:\n    branches: [main]\n  workflow_dispatch:\nNext, we define when the workflow is triggered. Here we have chosen:\n\npush: automatically run on pushes to the main branch.\nworkflow_dispatch: allows you to trigger the workflow manually from the GitHub actions interface (note: it only becomes available after first having been pushed to main).\n\njobs:\n  tests:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        include:\n          - os: ubuntu-latest\n            python-version: '3.11'\n          - os: ubuntu-latest\n            python-version: '3.12'\n          - os: windows-latest\n            python-version: '3.12'\n          - os: macos-latest\n            python-version: '3.12'\nNow we start to define the job that runs our tests. We are using matrix testing as this allows us to check our code across multiple operating systems and Python versions. In this case the tests will run on:\n\nPython 3.11 (Linux)\nPython 3.12 (Linux, Windows, macOS)\n\nThis is good as it allows you to spot any bugs/run issues related to specific operating systems or python versions. They will run in parallel, ensuring a single efficient workflow.\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\nNow we start defining the steps executed within our test job. The first step is typically to check out your repository, so the workflow can access your code.\n      - name: Install python and dependencies\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n          cache: 'pip'\nNext we install the version of Python specified in the matrix and enable pip caching to speed up future runs.\n      - name: Install requirements (Windows)\n        if: runner.os == 'Windows'\n        run: python -m pip install -r requirements-test.txt\n      \n      - name: Install requirements (Unix)\n        if: runner.os != 'Windows'\n        run: pip install -r requirements-test.txt\nDepending on the operating system, the command syntax for installing dependencies differs slightly. We use requirements-test.txt instead of environment.yaml as we want to use different Python versions. To reduce runtime, our requirements file only contains packages needed for running tests (e.g., excludes our linting packages).\n\n\n\n\n\n\nNoteSee requirements-test.txt\n\n\n\n\n\n\nnumpy==2.4.1\npandas==2.3.3\npip==25.3\npytest==9.0.2\npytest-cov==7.0.0\nscipy==1.17.0\n-e examples/python_package/.\n\n\n\n\n      - name: Run tests\n        run: pytest examples/python_package\nFinally, we run the tests! We call pytest on our case study (examples/python_package/). If all tests pass, you‚Äôll see green ticks for each environment - confirming that your code works consistently across Python versions and operating systems.\n\n\n\nThere are lots of ways to set up a testing action, depending on how your research is structured, and whether you are using an renv, and so on.\nIf your work is structured as a package, you can call:\nusethis::use_github_action()\nIt will prompt you to choose which action you would like:\n&gt; usethis::use_github_action()\nWhich action do you want to add? (0 to exit)\n(See &lt;https://github.com/r-lib/actions/tree/v2/examples&gt; for other options) \n\n1: check-standard: Run `R CMD check` on Linux, macOS, and Windows\n2: test-coverage: Compute test coverage and report to https://about.codecov.io\n\nSelection: \nHowever, for this tutorial, we will show an example where dependencies are restored from an renv and the tests are run via devtools::test(), running on ubuntu and specific version of R.\nHowever, you can do multiple operating systems! (Just see Python tutorial for how!)\n\nname: r_tests\nrun-name: Run R tests\n\non:\n  push:\n    branches: [main]\n  workflow_dispatch:\n\njobs:\n  tests:\n    runs-on: ubuntu-latest\n\n    env:\n      RENV_CONFIG_PAK_ENABLED: true\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          use-public-rspm: true\n          r-version: 4.4.1\n\n      - name: Restore renv from root\n        run: |\n          Rscript -e 'renv::restore(project = \".\")'\n\n      - name: Run testthat tests\n        run: |\n          Rscript -e 'renv::activate(); devtools::load_all(\"examples/r_package\"); testthat::test_dir(\"examples/r_package/tests/testthat\")'\n\n\nExplaining the workflow\nname: r_tests\nrun-name: Run R tests\nThe beginning of the YAML sets the workflow‚Äôs name and how it appears in the Actions tab.\n\nname is the internal name of the workflow file.\nrun-name is what is displayed when a run appears in the Actions history.\n\non:\n  push:\n    branches: [main]\n  workflow_dispatch:\nNext, we define when the workflow is triggered. Here we have chosen:\n\npush: automatically run on pushes to the main branch.\nworkflow_dispatch: allows you to trigger the workflow manually from the GitHub actions interface (note: it only becomes available after first having been pushed to main).\n\njobs:\n  tests:\n    runs-on: ubuntu-latest\n\n    env:\n      RENV_CONFIG_PAK_ENABLED: true\nNow we define the tests job. We specify an operating system (ubuntu-latest) and set RENV_CONFIG_PAK_ENABLED to true as it allows renv to use pak for faster, more reliable dependency installation where possible.\n    steps:\n      - uses: actions/checkout@v4\nThe first step checks out the repository so the workflow has access to the code.\n      - name: Set up R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          use-public-rspm: true\n          r-version: 4.4.1\n\n      - name: Restore renv from root\n        run: |\n          Rscript -e 'renv::restore(project = \".\")'\nThis step installs and configures R on the runner. The r-version is pinned to 4.4.1 here, with a specific renv then restored - but you could also run on latest versions.\n      - name: Run testthat tests\n        run: |\n          Rscript -e 'renv::activate()'\n          Rscript -e 'devtools::load_all(\"examples/r_package\")'\n          Rscript -e 'testthat::test_dir(\"examples/r_package/tests/testthat\")'\nFinally, the R tests are run. In this example, the research is structured as a package and in a subdirectory examples/r_package, so we load that with devtools before running the tests.",
    "crumbs": [
      "Running tests via GitHub actions"
    ]
  },
  {
    "objectID": "pages/github_actions.html#see-github-actions-in-action",
    "href": "pages/github_actions.html#see-github-actions-in-action",
    "title": "Running tests via GitHub actions",
    "section": "See GitHub actions, in action!",
    "text": "See GitHub actions, in action!\nThe video below demonstrates this workflow running in GitHub Actions. For the demo, the workflow is triggered manually using workflow_dispatch, but it would also run automatically whenever you push changes to main. In the video you‚Äôll see we:\n\n\nOpen the GitHub repository.\nGo to the Actions tab.\nClick on the python_tests action.\nView the workflow YAML.\nTrigger the action via workflow_dispatch.\nView the running test - see they are installing the requirements, running tests, and pass.\n\n\nThe tests all pass in the end-\n\n\n\nTODO",
    "crumbs": [
      "Running tests via GitHub actions"
    ]
  }
]